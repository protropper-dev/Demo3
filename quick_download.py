#!/usr/bin/env python3
"""
Script t·∫£i nhanh c√°c m√¥ h√¨nh AI
S·ª≠ d·ª•ng: python quick_download.py
"""

import os
import sys
from pathlib import Path

# Th√™m backend1 v√†o Python path ƒë·ªÉ import paths
backend_path = Path(__file__).parent / "backend1"
sys.path.insert(0, str(backend_path))

from app.core.paths import MODELS_DIR, EMBEDDING_MODEL, LLM_MODEL

def quick_download():
    """T·∫£i nhanh c·∫£ 2 m√¥ h√¨nh"""
    print("üöÄ B·∫Øt ƒë·∫ßu t·∫£i nhanh c√°c m√¥ h√¨nh AI...")
    
    try:
        from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
        from huggingface_hub import snapshot_download
        print("‚úÖ Dependencies ƒë√£ s·∫µn s√†ng")
    except ImportError as e:
        print(f"‚ùå Thi·∫øu dependencies: {e}")
        print("Vui l√≤ng ch·∫°y: pip install transformers torch huggingface_hub")
        return False
    
    # S·ª≠ d·ª•ng paths.py ƒë·ªÉ l·∫•y ƒë∆∞·ªùng d·∫´n models
    models_dir = MODELS_DIR
    models_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üìÅ Models directory: {models_dir}")
    
    # C·∫•u h√¨nh m√¥ h√¨nh - s·ª≠ d·ª•ng paths.py
    models = [
        {
            "name": "multilingual_e5_large",
            "repo_id": "intfloat/multilingual-e5-large",
            "local_dir": EMBEDDING_MODEL
        },
        {
            "name": "vinallama-2.7b-chat", 
            "repo_id": "Viet-Mistral/Vinallama-2.7B-Chat",
            "local_dir": LLM_MODEL
        }
    ]
    
    results = {}
    
    for model in models:
        print(f"\nüì¶ ƒêang t·∫£i: {model['name']}")
        print(f"   T·ª´: {model['repo_id']}")
        print(f"   ƒê·∫øn: {model['local_dir']}")
        
        try:
            # T·∫£i m√¥ h√¨nh
            snapshot_download(
                repo_id=model["repo_id"],
                local_dir=str(model["local_dir"]),
                local_dir_use_symlinks=False,
                resume_download=True
            )
            
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc
            total_size = sum(f.stat().st_size for f in model["local_dir"].rglob('*') if f.is_file())
            size_gb = total_size / (1024**3)
            
            print(f"‚úÖ Th√†nh c√¥ng: {model['name']} ({size_gb:.2f} GB)")
            results[model["name"]] = True
            
        except Exception as e:
            print(f"‚ùå Th·∫•t b·∫°i: {model['name']} - {str(e)}")
            results[model["name"]] = False
    
    # T·ªïng k·∫øt
    print(f"\nüìä K·∫æT QU·∫¢:")
    for name, success in results.items():
        status = "‚úÖ Th√†nh c√¥ng" if success else "‚ùå Th·∫•t b·∫°i"
        print(f"   - {name}: {status}")
    
    success_count = sum(results.values())
    total_count = len(results)
    
    if success_count == total_count:
        print(f"\nüéâ Ho√†n th√†nh! ƒê√£ t·∫£i th√†nh c√¥ng {success_count}/{total_count} m√¥ h√¨nh")
        print(f"üìÅ Th∆∞ m·ª•c: {models_dir.absolute()}")
    else:
        print(f"\n‚ö†Ô∏è Ho√†n th√†nh m·ªôt ph·∫ßn: {success_count}/{total_count} m√¥ h√¨nh")
    
    return success_count == total_count

if __name__ == "__main__":
    quick_download()
