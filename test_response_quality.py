#!/usr/bin/env python3
"""
Response Quality Testing Script cho H·ªá th·ªëng Tr·ª£ l√Ω ·∫¢o An to√†n Th√¥ng tin
ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi qua c√°c ti√™u ch√≠: coherence, relevance, helpfulness, clarity
"""

import asyncio
import aiohttp
import json
import time
import re
from datetime import datetime
from typing import List, Dict, Any, Tuple
import logging
from dataclasses import dataclass

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('response_quality_test_results.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class QualityTestCase:
    """Test case cho response quality testing"""
    question: str
    context: str  # Ng·ªØ c·∫£nh c·ªßa c√¢u h·ªèi
    expected_response_type: str  # factual, procedural, comparative, explanatory
    expected_tone: str  # professional, educational, technical, friendly
    complexity_level: str  # basic, intermediate, advanced
    user_type: str  # beginner, expert, administrator, student

class ResponseQualityTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.api_endpoint = f"{base_url}/api/v1/rag/query"
        self.session = None
        
        # T·∫°o b·ªô test cases cho quality testing
        self.test_cases = self._create_quality_test_cases()
    
    def _create_quality_test_cases(self) -> List[QualityTestCase]:
        """T·∫°o b·ªô test cases ƒëa d·∫°ng cho quality testing"""
        return [
            # Test cases cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu
            QualityTestCase(
                question="T√¥i m·ªõi l√†m vi·ªác v·ªõi an to√†n th√¥ng tin, b·∫°n c√≥ th·ªÉ gi·∫£i th√≠ch ƒë∆°n gi·∫£n kh√¥ng?",
                context="Ng∆∞·ªùi d√πng m·ªõi b·∫Øt ƒë·∫ßu t√¨m hi·ªÉu v·ªÅ an to√†n th√¥ng tin",
                expected_response_type="explanatory",
                expected_tone="educational",
                complexity_level="basic",
                user_type="beginner"
            ),
            QualityTestCase(
                question="T·∫°i sao t√¥i c·∫ßn quan t√¢m ƒë·∫øn an to√†n th√¥ng tin?",
                context="C√¢u h·ªèi t·ª´ ng∆∞·ªùi ch∆∞a hi·ªÉu t·∫ßm quan tr·ªçng c·ªßa an to√†n th√¥ng tin",
                expected_response_type="explanatory",
                expected_tone="educational",
                complexity_level="basic",
                user_type="beginner"
            ),
            
            # Test cases cho chuy√™n gia k·ªπ thu·∫≠t
            QualityTestCase(
                question="So s√°nh hi·ªáu qu·∫£ gi·ªØa AES-256 v√† ChaCha20-Poly1305 trong vi·ªác m√£ h√≥a d·ªØ li·ªáu nh·∫°y c·∫£m",
                context="Chuy√™n gia b·∫£o m·∫≠t c·∫ßn th√¥ng tin k·ªπ thu·∫≠t chi ti·∫øt",
                expected_response_type="comparative",
                expected_tone="technical",
                complexity_level="advanced",
                user_type="expert"
            ),
            QualityTestCase(
                question="C√°ch tri·ªÉn khai Zero Trust Architecture trong m√¥i tr∆∞·ªùng hybrid cloud?",
                context="Ki·∫øn tr√∫c s∆∞ b·∫£o m·∫≠t c·∫ßn h∆∞·ªõng d·∫´n tri·ªÉn khai",
                expected_response_type="procedural",
                expected_tone="technical",
                complexity_level="advanced",
                user_type="expert"
            ),
            
            # Test cases cho qu·∫£n tr·ªã vi√™n h·ªá th·ªëng
            QualityTestCase(
                question="T√¥i c·∫ßn thi·∫øt l·∫≠p ch√≠nh s√°ch m·∫≠t kh·∫©u cho 500 nh√¢n vi√™n, b·∫°n c√≥ g·ª£i √Ω g√¨?",
                context="Qu·∫£n tr·ªã vi√™n c·∫ßn h∆∞·ªõng d·∫´n th·ª±c t·∫ø",
                expected_response_type="procedural",
                expected_tone="professional",
                complexity_level="intermediate",
                user_type="administrator"
            ),
            QualityTestCase(
                question="C√°c b∆∞·ªõc x·ª≠ l√Ω khi ph√°t hi·ªán ransomware trong h·ªá th·ªëng?",
                context="S·ª± c·ªë kh·∫©n c·∫•p c·∫ßn h∆∞·ªõng d·∫´n r√µ r√†ng",
                expected_response_type="procedural",
                expected_tone="professional",
                complexity_level="intermediate",
                user_type="administrator"
            ),
            
            # Test cases cho sinh vi√™n/h·ªçc vi√™n
            QualityTestCase(
                question="B·∫°n c√≥ th·ªÉ gi·∫£i th√≠ch kh√°i ni·ªám CIA triad trong an to√†n th√¥ng tin kh√¥ng?",
                context="Sinh vi√™n h·ªçc v·ªÅ nguy√™n t·∫Øc c∆° b·∫£n",
                expected_response_type="explanatory",
                expected_tone="educational",
                complexity_level="basic",
                user_type="student"
            ),
            QualityTestCase(
                question="V√≠ d·ª• th·ª±c t·∫ø v·ªÅ vi ph·∫°m an to√†n th√¥ng tin v√† h·∫≠u qu·∫£ c·ªßa n√≥?",
                context="H·ªçc vi√™n c·∫ßn v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ hi·ªÉu",
                expected_response_type="explanatory",
                expected_tone="educational",
                complexity_level="intermediate",
                user_type="student"
            ),
            
            # Test cases ki·ªÉm tra kh·∫£ nƒÉng x·ª≠ l√Ω c√¢u h·ªèi ph·ª©c t·∫°p
            QualityTestCase(
                question="L√†m th·∫ø n√†o ƒë·ªÉ ƒë√°nh gi√° v√† c·∫£i thi·ªán ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o nh·∫≠n th·ª©c an to√†n th√¥ng tin cho t·ªï ch·ª©c?",
                context="C√¢u h·ªèi ph·ª©c t·∫°p ƒë√≤i h·ªèi ph√¢n t√≠ch v√† ƒë∆∞a ra gi·∫£i ph√°p",
                expected_response_type="procedural",
                expected_tone="professional",
                complexity_level="advanced",
                user_type="administrator"
            ),
            QualityTestCase(
                question="Ph√¢n t√≠ch ∆∞u nh∆∞·ª£c ƒëi·ªÉm c·ªßa vi·ªác s·ª≠ d·ª•ng AI trong ph√°t hi·ªán v√† ngƒÉn ch·∫∑n m·ªëi ƒëe d·ªça an to√†n th√¥ng tin",
                context="C√¢u h·ªèi ph√¢n t√≠ch ƒë√≤i h·ªèi t∆∞ duy ph·∫£n bi·ªán",
                expected_response_type="comparative",
                expected_tone="technical",
                complexity_level="advanced",
                user_type="expert"
            ),
            
            # Test cases ki·ªÉm tra kh·∫£ nƒÉng th√≠ch ·ª©ng v·ªõi ng·ªØ c·∫£nh
            QualityTestCase(
                question="T√¥i ƒëang l√†m vi·ªác t·∫°i ng√¢n h√†ng, c√°c quy ƒë·ªãnh an to√†n th√¥ng tin n√†o t√¥i c·∫ßn tu√¢n th·ªß?",
                context="Ng·ªØ c·∫£nh c·ª• th·ªÉ v·ªÅ lƒ©nh v·ª±c ng√¢n h√†ng",
                expected_response_type="factual",
                expected_tone="professional",
                complexity_level="intermediate",
                user_type="administrator"
            ),
            QualityTestCase(
                question="Kh√°c bi·ªát gi·ªØa GDPR v√† Lu·∫≠t An to√†n th√¥ng tin Vi·ªát Nam v·ªÅ b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n?",
                context="So s√°nh quy ƒë·ªãnh qu·ªëc t·∫ø v√† trong n∆∞·ªõc",
                expected_response_type="comparative",
                expected_tone="professional",
                complexity_level="advanced",
                user_type="expert"
            )
        ]
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def test_single_quality_case(self, test_case: QualityTestCase) -> Dict[str, Any]:
        """Test m·ªôt quality test case ƒë∆°n l·∫ª"""
        payload = {
            "question": test_case.question,
            "top_k": 5,
            "include_sources": True,
            "use_enhancement": True
        }
        
        try:
            async with self.session.post(self.api_endpoint, json=payload) as response:
                if response.status != 200:
                    return {
                        "test_case": test_case.question,
                        "success": False,
                        "error": f"HTTP {response.status}",
                        "quality_metrics": {}
                    }
                
                response_data = await response.json()
                answer = response_data.get("answer", "")
                sources = response_data.get("sources", [])
                
                # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi
                quality_metrics = self._evaluate_response_quality(test_case, answer, sources)
                
                return {
                    "test_case": test_case.question,
                    "context": test_case.context,
                    "success": True,
                    "answer": answer,
                    "sources": sources,
                    "quality_metrics": quality_metrics,
                    "response_data": response_data
                }
        
        except Exception as e:
            logger.error(f"Quality test case failed: {test_case.question} - {e}")
            return {
                "test_case": test_case.question,
                "success": False,
                "error": str(e),
                "quality_metrics": {}
            }
    
    def _evaluate_response_quality(self, test_case: QualityTestCase, answer: str, sources: List[Dict]) -> Dict[str, Any]:
        """ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi"""
        
        # 1. Coherence - T√≠nh m·∫°ch l·∫°c v√† logic
        coherence_score = self._calculate_coherence(answer)
        
        # 2. Relevance - T√≠nh li√™n quan ƒë·∫øn c√¢u h·ªèi
        relevance_score = self._calculate_relevance(test_case.question, answer)
        
        # 3. Helpfulness - T√≠nh h·ªØu √≠ch cho ng∆∞·ªùi d√πng
        helpfulness_score = self._calculate_helpfulness(test_case, answer)
        
        # 4. Clarity - T√≠nh r√µ r√†ng v√† d·ªÖ hi·ªÉu
        clarity_score = self._calculate_clarity(answer, test_case.complexity_level)
        
        # 5. Completeness - T√≠nh ƒë·∫ßy ƒë·ªß
        completeness_score = self._calculate_completeness(test_case, answer)
        
        # 6. Tone Appropriateness - T√≠nh ph√π h·ª£p c·ªßa gi·ªçng ƒëi·ªáu
        tone_score = self._calculate_tone_appropriateness(answer, test_case.expected_tone)
        
        # 7. Source Quality - Ch·∫•t l∆∞·ª£ng ngu·ªìn tham kh·∫£o
        source_quality_score = self._calculate_source_quality(sources)
        
        # 8. Structure - C·∫•u tr√∫c v√† t·ªï ch·ª©c th√¥ng tin
        structure_score = self._calculate_structure(answer)
        
        # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p
        overall_score = (
            coherence_score * 0.15 +
            relevance_score * 0.15 +
            helpfulness_score * 0.15 +
            clarity_score * 0.15 +
            completeness_score * 0.15 +
            tone_score * 0.10 +
            source_quality_score * 0.10 +
            structure_score * 0.05
        )
        
        return {
            "coherence": coherence_score,
            "relevance": relevance_score,
            "helpfulness": helpfulness_score,
            "clarity": clarity_score,
            "completeness": completeness_score,
            "tone_appropriateness": tone_score,
            "source_quality": source_quality_score,
            "structure": structure_score,
            "overall_score": overall_score,
            "answer_length": len(answer),
            "sources_count": len(sources),
            "complexity_level": test_case.complexity_level,
            "user_type": test_case.user_type,
            "expected_tone": test_case.expected_tone
        }
    
    def _calculate_coherence(self, answer: str) -> float:
        """T√≠nh t√≠nh m·∫°ch l·∫°c c·ªßa c√¢u tr·∫£ l·ªùi"""
        if not answer:
            return 0.0
        
        # Ki·ªÉm tra c·∫•u tr√∫c c√¢u v√† ƒëo·∫°n vƒÉn
        sentences = re.split(r'[.!?]+', answer)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) < 2:
            return 0.5
        
        # Ki·ªÉm tra t·ª´ n·ªëi logic
        logical_connectors = [
            "do ƒë√≥", "v√¨ v·∫≠y", "tuy nhi√™n", "ngo√†i ra", "ƒë·ªìng th·ªùi",
            "tr∆∞·ªõc ti√™n", "ti·∫øp theo", "cu·ªëi c√πng", "t·ªïng k·∫øt",
            "v·ªÅ m·∫∑t", "x√©t v·ªÅ", "c·ª• th·ªÉ", "v√≠ d·ª•", "nh∆∞ v·∫≠y"
        ]
        
        connector_count = sum(1 for connector in logical_connectors if connector in answer.lower())
        connector_score = min(connector_count / 3, 1.0)  # T·ªëi ƒëa 3 t·ª´ n·ªëi
        
        # Ki·ªÉm tra ƒë·ªô d√†i c√¢u h·ª£p l√Ω
        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
        length_score = 1.0 if 20 <= avg_sentence_length <= 100 else 0.7
        
        return (connector_score + length_score) / 2
    
    def _calculate_relevance(self, question: str, answer: str) -> float:
        """T√≠nh t√≠nh li√™n quan ƒë·∫øn c√¢u h·ªèi"""
        if not answer or not question:
            return 0.0
        
        # Tr√≠ch xu·∫•t t·ª´ kh√≥a ch√≠nh t·ª´ c√¢u h·ªèi
        question_keywords = self._extract_keywords(question)
        answer_keywords = self._extract_keywords(answer)
        
        if not question_keywords:
            return 0.5
        
        # T√≠nh ƒë·ªô tr√πng l·∫∑p t·ª´ kh√≥a
        common_keywords = set(question_keywords) & set(answer_keywords)
        keyword_relevance = len(common_keywords) / len(question_keywords)
        
        # Ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c√≥ tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi kh√¥ng
        question_type_indicators = {
            "what": ["l√† g√¨", "l√†", "ƒë·ªãnh nghƒ©a", "kh√°i ni·ªám"],
            "how": ["nh∆∞ th·∫ø n√†o", "c√°ch", "l√†m sao", "quy tr√¨nh"],
            "why": ["t·∫°i sao", "l√Ω do", "nguy√™n nh√¢n"],
            "when": ["khi n√†o", "th·ªùi gian", "l√∫c n√†o"],
            "where": ["·ªü ƒë√¢u", "n∆°i n√†o", "ƒë·ªãa ƒëi·ªÉm"]
        }
        
        question_lower = question.lower()
        answer_lower = answer.lower()
        
        direct_answer_score = 0.0
        for q_type, indicators in question_type_indicators.items():
            if any(indicator in question_lower for indicator in indicators):
                # Ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c√≥ ch·ª©a t·ª´ kh√≥a li√™n quan kh√¥ng
                if any(indicator in answer_lower for indicator in indicators):
                    direct_answer_score = 1.0
                    break
                else:
                    direct_answer_score = 0.7
        
        return (keyword_relevance + direct_answer_score) / 2
    
    def _calculate_helpfulness(self, test_case: QualityTestCase, answer: str) -> float:
        """T√≠nh t√≠nh h·ªØu √≠ch cho ng∆∞·ªùi d√πng"""
        if not answer:
            return 0.0
        
        helpfulness_score = 0.0
        
        # Ki·ªÉm tra c√≥ cung c·∫•p th√¥ng tin th·ª±c t·∫ø kh√¥ng
        if len(answer) > 50:  # ƒê·ªô d√†i t·ªëi thi·ªÉu
            helpfulness_score += 0.3
        
        # Ki·ªÉm tra c√≥ v√≠ d·ª• c·ª• th·ªÉ kh√¥ng
        example_indicators = ["v√≠ d·ª•", "ch·∫≥ng h·∫°n", "c·ª• th·ªÉ", "th·ª±c t·∫ø", "nh∆∞"]
        if any(indicator in answer.lower() for indicator in example_indicators):
            helpfulness_score += 0.2
        
        # Ki·ªÉm tra c√≥ h∆∞·ªõng d·∫´n th·ª±c hi·ªán kh√¥ng (cho procedural questions)
        if test_case.expected_response_type == "procedural":
            action_indicators = ["b∆∞·ªõc", "th·ª±c hi·ªán", "ti·∫øn h√†nh", "tri·ªÉn khai", "c√†i ƒë·∫∑t"]
            if any(indicator in answer.lower() for indicator in action_indicators):
                helpfulness_score += 0.3
            else:
                helpfulness_score += 0.1
        else:
            helpfulness_score += 0.2
        
        # Ki·ªÉm tra c√≥ ngu·ªìn tham kh·∫£o kh√¥ng
        if "ngu·ªìn" in answer.lower() or "tham kh·∫£o" in answer.lower():
            helpfulness_score += 0.1
        
        # Ki·ªÉm tra c√≥ c·∫£nh b√°o ho·∫∑c l∆∞u √Ω quan tr·ªçng kh√¥ng
        warning_indicators = ["l∆∞u √Ω", "c·∫£nh b√°o", "quan tr·ªçng", "ch√∫ √Ω", "c·∫ßn thi·∫øt"]
        if any(indicator in answer.lower() for indicator in warning_indicators):
            helpfulness_score += 0.1
        
        return min(helpfulness_score, 1.0)
    
    def _calculate_clarity(self, answer: str, complexity_level: str) -> float:
        """T√≠nh t√≠nh r√µ r√†ng v√† d·ªÖ hi·ªÉu"""
        if not answer:
            return 0.0
        
        clarity_score = 0.0
        
        # Ki·ªÉm tra c·∫•u tr√∫c ƒëo·∫°n vƒÉn
        paragraphs = [p.strip() for p in answer.split('\n') if p.strip()]
        if len(paragraphs) > 1:
            clarity_score += 0.2
        
        # Ki·ªÉm tra ƒë·ªô d√†i c√¢u ph√π h·ª£p v·ªõi ƒë·ªô ph·ª©c t·∫°p
        sentences = re.split(r'[.!?]+', answer)
        avg_sentence_length = sum(len(s.strip()) for s in sentences if s.strip()) / len(sentences)
        
        if complexity_level == "basic":
            if avg_sentence_length <= 50:
                clarity_score += 0.3
            else:
                clarity_score += 0.1
        elif complexity_level == "intermediate":
            if 30 <= avg_sentence_length <= 80:
                clarity_score += 0.3
            else:
                clarity_score += 0.1
        else:  # advanced
            if 40 <= avg_sentence_length <= 120:
                clarity_score += 0.3
            else:
                clarity_score += 0.1
        
        # Ki·ªÉm tra t·ª´ kh√≥ hi·ªÉu (technical jargon kh√¥ng c·∫ßn thi·∫øt)
        complex_terms = ["paradigm", "infrastructure", "implementation", "architecture"]
        complex_count = sum(1 for term in complex_terms if term.lower() in answer.lower())
        if complexity_level == "basic" and complex_count > 2:
            clarity_score -= 0.1
        elif complexity_level == "advanced" and complex_count < 1:
            clarity_score -= 0.1
        
        # Ki·ªÉm tra c√≥ gi·∫£i th√≠ch thu·∫≠t ng·ªØ kh√¥ng
        explanation_indicators = ["nghƒ©a l√†", "t·ª©c l√†", "c√≥ th·ªÉ hi·ªÉu", "ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a"]
        if any(indicator in answer.lower() for indicator in explanation_indicators):
            clarity_score += 0.2
        
        return max(min(clarity_score, 1.0), 0.0)
    
    def _calculate_completeness(self, test_case: QualityTestCase, answer: str) -> float:
        """T√≠nh t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi"""
        if not answer:
            return 0.0
        
        completeness_score = 0.0
        
        # Ki·ªÉm tra ƒë·ªô d√†i ph√π h·ª£p v·ªõi lo·∫°i c√¢u h·ªèi
        if test_case.expected_response_type == "factual":
            if len(answer) >= 100:
                completeness_score += 0.3
        elif test_case.expected_response_type == "procedural":
            if len(answer) >= 200:
                completeness_score += 0.3
        elif test_case.expected_response_type == "comparative":
            if len(answer) >= 250:
                completeness_score += 0.3
        elif test_case.expected_response_type == "explanatory":
            if len(answer) >= 150:
                completeness_score += 0.3
        
        # Ki·ªÉm tra c√≥ ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c kh√≠a c·∫°nh kh√°c nhau kh√¥ng
        aspect_indicators = ["ƒë·∫ßu ti√™n", "th·ª© hai", "ngo√†i ra", "m·∫∑t kh√°c", "ƒë·ªìng th·ªùi"]
        aspect_count = sum(1 for indicator in aspect_indicators if indicator in answer.lower())
        completeness_score += min(aspect_count * 0.1, 0.3)
        
        # Ki·ªÉm tra c√≥ k·∫øt lu·∫≠n ho·∫∑c t√≥m t·∫Øt kh√¥ng
        conclusion_indicators = ["t√≥m l·∫°i", "k·∫øt lu·∫≠n", "t·ªïng k·∫øt", "nh∆∞ v·∫≠y", "cu·ªëi c√πng"]
        if any(indicator in answer.lower() for indicator in conclusion_indicators):
            completeness_score += 0.2
        
        # Ki·ªÉm tra c√≥ ƒë·ªÅ c·∫≠p ƒë·∫øn ngu·ªìn tham kh·∫£o kh√¥ng
        if "theo" in answer.lower() or "ngu·ªìn" in answer.lower():
            completeness_score += 0.2
        
        return min(completeness_score, 1.0)
    
    def _calculate_tone_appropriateness(self, answer: str, expected_tone: str) -> float:
        """T√≠nh t√≠nh ph√π h·ª£p c·ªßa gi·ªçng ƒëi·ªáu"""
        if not answer:
            return 0.0
        
        tone_indicators = {
            "professional": ["theo quy ƒë·ªãnh", "c·∫ßn thi·∫øt", "b·∫Øt bu·ªôc", "tu√¢n th·ªß", "th·ª±c hi·ªán"],
            "educational": ["h√£y c√πng", "b·∫°n c√≥ th·ªÉ", "ƒë·ªÉ hi·ªÉu", "gi·∫£i th√≠ch", "v√≠ d·ª•"],
            "technical": ["tri·ªÉn khai", "c·∫•u h√¨nh", "thu·∫≠t to√°n", "giao th·ª©c", "ki·∫øn tr√∫c"],
            "friendly": ["b·∫°n", "t√¥i", "ch√∫ng ta", "c√πng nhau", "h√£y"]
        }
        
        answer_lower = answer.lower()
        expected_indicators = tone_indicators.get(expected_tone, [])
        
        if not expected_indicators:
            return 0.5
        
        indicator_count = sum(1 for indicator in expected_indicators if indicator in answer_lower)
        return min(indicator_count / len(expected_indicators) * 2, 1.0)
    
    def _calculate_source_quality(self, sources: List[Dict]) -> float:
        """T√≠nh ch·∫•t l∆∞·ª£ng ngu·ªìn tham kh·∫£o"""
        if not sources:
            return 0.0
        
        quality_score = 0.0
        
        # Ki·ªÉm tra s·ªë l∆∞·ª£ng ngu·ªìn
        if len(sources) >= 2:
            quality_score += 0.3
        elif len(sources) >= 1:
            quality_score += 0.2
        
        # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng ngu·ªìn (d·ª±a tr√™n t√™n file v√† category)
        for source in sources:
            filename = source.get("filename", "").lower()
            category = source.get("category", "")
            similarity = source.get("similarity_score", 0)
            
            # ƒêi·ªÉm cho ngu·ªìn c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao
            if similarity > 0.7:
                quality_score += 0.2
            elif similarity > 0.5:
                quality_score += 0.1
            
            # ƒêi·ªÉm cho ngu·ªìn uy t√≠n (ISO, NIST, Lu·∫≠t, TCVN)
            if any(keyword in filename for keyword in ["iso", "nist", "luat", "tcvn", "ngh·ªã ƒë·ªãnh"]):
                quality_score += 0.2
        
        return min(quality_score, 1.0)
    
    def _calculate_structure(self, answer: str) -> float:
        """T√≠nh c·∫•u tr√∫c v√† t·ªï ch·ª©c th√¥ng tin"""
        if not answer:
            return 0.0
        
        structure_score = 0.0
        
        # Ki·ªÉm tra c√≥ ti√™u ƒë·ªÅ ho·∫∑c ƒë√°nh s·ªë kh√¥ng
        if re.search(r'^\d+\.', answer, re.MULTILINE) or re.search(r'^[A-Z][a-z]+:', answer, re.MULTILINE):
            structure_score += 0.3
        
        # Ki·ªÉm tra c√≥ danh s√°ch kh√¥ng
        if re.search(r'[-‚Ä¢*]\s', answer) or re.search(r'\d+\.\s', answer):
            structure_score += 0.2
        
        # Ki·ªÉm tra c√≥ ƒëo·∫°n vƒÉn ri√™ng bi·ªát kh√¥ng
        paragraphs = [p.strip() for p in answer.split('\n\n') if p.strip()]
        if len(paragraphs) > 1:
            structure_score += 0.3
        
        # Ki·ªÉm tra c√≥ t·ª´ n·ªëi gi·ªØa c√°c √Ω kh√¥ng
        transition_words = ["ƒë·∫ßu ti√™n", "ti·∫øp theo", "ngo√†i ra", "tuy nhi√™n", "do ƒë√≥"]
        if any(word in answer.lower() for word in transition_words):
            structure_score += 0.2
        
        return min(structure_score, 1.0)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ vƒÉn b·∫£n"""
        # Lo·∫°i b·ªè d·∫•u c√¢u v√† chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
        clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
        
        # T√°ch t·ª´
        words = clean_text.split()
        
        # Lo·∫°i b·ªè t·ª´ d·ª´ng ti·∫øng Vi·ªát
        stop_words = {
            "l√†", "c·ªßa", "v√†", "v·ªõi", "trong", "cho", "ƒë·∫øn", "t·ª´", "c√≥", "ƒë∆∞·ª£c",
            "n√†y", "ƒë√≥", "c√°c", "m·ªôt", "nh∆∞", "v·ªÅ", "s·∫Ω", "ƒë√£", "ƒëang", "b·ªã"
        }
        
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        return keywords[:10]  # L·∫•y 10 t·ª´ kh√≥a quan tr·ªçng nh·∫•t
    
    async def run_all_quality_tests(self) -> List[Dict[str, Any]]:
        """Ch·∫°y t·∫•t c·∫£ quality test cases"""
        logger.info(f"üß™ B·∫Øt ƒë·∫ßu ch·∫°y {len(self.test_cases)} quality test cases...")
        
        all_results = []
        for i, test_case in enumerate(self.test_cases, 1):
            logger.info(f"Quality test case {i}/{len(self.test_cases)}: {test_case.question[:50]}...")
            result = await self.test_single_quality_case(test_case)
            all_results.append(result)
            
            # Ngh·ªâ ng·∫Øn gi·ªØa c√°c test
            await asyncio.sleep(0.5)
        
        return all_results
    
    def calculate_quality_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """T√≠nh to√°n c√°c metric ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ"""
        successful_results = [r for r in results if r.get("success", False)]
        failed_results = [r for r in results if not r.get("success", False)]
        
        if not successful_results:
            return {"error": "Kh√¥ng c√≥ quality test case n√†o th√†nh c√¥ng"}
        
        # T√≠nh to√°n metrics t·ªïng th·ªÉ
        all_scores = [r["quality_metrics"]["overall_score"] for r in successful_results]
        coherence_scores = [r["quality_metrics"]["coherence"] for r in successful_results]
        relevance_scores = [r["quality_metrics"]["relevance"] for r in successful_results]
        helpfulness_scores = [r["quality_metrics"]["helpfulness"] for r in successful_results]
        clarity_scores = [r["quality_metrics"]["clarity"] for r in successful_results]
        completeness_scores = [r["quality_metrics"]["completeness"] for r in successful_results]
        tone_scores = [r["quality_metrics"]["tone_appropriateness"] for r in successful_results]
        source_scores = [r["quality_metrics"]["source_quality"] for r in successful_results]
        structure_scores = [r["quality_metrics"]["structure"] for r in successful_results]
        
        # Ph√¢n t√≠ch theo ƒë·ªô ph·ª©c t·∫°p
        complexity_analysis = {}
        for complexity in ["basic", "intermediate", "advanced"]:
            complexity_results = [r for r in successful_results if r["quality_metrics"]["complexity_level"] == complexity]
            if complexity_results:
                scores = [r["quality_metrics"]["overall_score"] for r in complexity_results]
                complexity_analysis[complexity] = {
                    "count": len(complexity_results),
                    "avg_score": sum(scores) / len(scores),
                    "min_score": min(scores),
                    "max_score": max(scores)
                }
        
        # Ph√¢n t√≠ch theo lo·∫°i ng∆∞·ªùi d√πng
        user_type_analysis = {}
        for user_type in ["beginner", "expert", "administrator", "student"]:
            user_type_results = [r for r in successful_results if r["quality_metrics"]["user_type"] == user_type]
            if user_type_results:
                scores = [r["quality_metrics"]["overall_score"] for r in user_type_results]
                user_type_analysis[user_type] = {
                    "count": len(user_type_results),
                    "avg_score": sum(scores) / len(scores),
                    "min_score": min(scores),
                    "max_score": max(scores)
                }
        
        return {
            "total_tests": len(results),
            "successful_tests": len(successful_results),
            "failed_tests": len(failed_results),
            "success_rate": len(successful_results) / len(results) * 100,
            
            "overall_quality_scores": {
                "mean": sum(all_scores) / len(all_scores),
                "min": min(all_scores),
                "max": max(all_scores),
                "std_dev": self._calculate_std_dev(all_scores)
            },
            
            "component_scores": {
                "coherence": {
                    "mean": sum(coherence_scores) / len(coherence_scores),
                    "min": min(coherence_scores),
                    "max": max(coherence_scores)
                },
                "relevance": {
                    "mean": sum(relevance_scores) / len(relevance_scores),
                    "min": min(relevance_scores),
                    "max": max(relevance_scores)
                },
                "helpfulness": {
                    "mean": sum(helpfulness_scores) / len(helpfulness_scores),
                    "min": min(helpfulness_scores),
                    "max": max(helpfulness_scores)
                },
                "clarity": {
                    "mean": sum(clarity_scores) / len(clarity_scores),
                    "min": min(clarity_scores),
                    "max": max(clarity_scores)
                },
                "completeness": {
                    "mean": sum(completeness_scores) / len(completeness_scores),
                    "min": min(completeness_scores),
                    "max": max(completeness_scores)
                },
                "tone_appropriateness": {
                    "mean": sum(tone_scores) / len(tone_scores),
                    "min": min(tone_scores),
                    "max": max(tone_scores)
                },
                "source_quality": {
                    "mean": sum(source_scores) / len(source_scores),
                    "min": min(source_scores),
                    "max": max(source_scores)
                },
                "structure": {
                    "mean": sum(structure_scores) / len(structure_scores),
                    "min": min(structure_scores),
                    "max": max(structure_scores)
                }
            },
            
            "complexity_analysis": complexity_analysis,
            "user_type_analysis": user_type_analysis
        }
    
    @staticmethod
    def _calculate_std_dev(scores: List[float]) -> float:
        """T√≠nh ƒë·ªô l·ªách chu·∫©n"""
        if len(scores) < 2:
            return 0.0
        
        mean = sum(scores) / len(scores)
        variance = sum((x - mean) ** 2 for x in scores) / (len(scores) - 1)
        return variance ** 0.5
    
    def save_results(self, metrics: Dict[str, Any], detailed_results: List[Dict[str, Any]]):
        """L∆∞u k·∫øt qu·∫£ test"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"response_quality_test_results_{timestamp}.json"
        
        output = {
            "test_type": "response_quality_testing",
            "timestamp": timestamp,
            "metrics": metrics,
            "detailed_results": detailed_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üìä K·∫øt qu·∫£ test ƒë√£ ƒë∆∞·ª£c l∆∞u: {filename}")
        return filename
    
    def print_summary(self, metrics: Dict[str, Any]):
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        print(f"\n{'='*60}")
        print(f"üìù K·∫æT QU·∫¢ TH·ª¨ NGHI·ªÜM CH·∫§T L∆Ø·ª¢NG PH·∫¢N H·ªíI")
        print(f"{'='*60}")
        
        if "error" in metrics:
            print(f"‚ùå L·ªói: {metrics['error']}")
            return
        
        print(f"üìä T·ªïng quan:")
        print(f"   ‚Ä¢ T·ªïng test cases: {metrics['total_tests']}")
        print(f"   ‚Ä¢ Th√†nh c√¥ng: {metrics['successful_tests']}")
        print(f"   ‚Ä¢ Th·∫•t b·∫°i: {metrics['failed_tests']}")
        print(f"   ‚Ä¢ T·ª∑ l·ªá th√†nh c√¥ng: {metrics['success_rate']:.2f}%")
        
        print(f"\nüìù ƒêi·ªÉm ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ:")
        overall = metrics['overall_quality_scores']
        print(f"   ‚Ä¢ ƒêi·ªÉm trung b√¨nh: {overall['mean']:.3f}/1.0")
        print(f"   ‚Ä¢ ƒêi·ªÉm th·∫•p nh·∫•t: {overall['min']:.3f}")
        print(f"   ‚Ä¢ ƒêi·ªÉm cao nh·∫•t: {overall['max']:.3f}")
        print(f"   ‚Ä¢ ƒê·ªô l·ªách chu·∫©n: {overall['std_dev']:.3f}")
        
        print(f"\nüéØ Ph√¢n t√≠ch th√†nh ph·∫ßn:")
        components = metrics['component_scores']
        print(f"   ‚Ä¢ M·∫°ch l·∫°c: {components['coherence']['mean']:.3f}")
        print(f"   ‚Ä¢ Li√™n quan: {components['relevance']['mean']:.3f}")
        print(f"   ‚Ä¢ H·ªØu √≠ch: {components['helpfulness']['mean']:.3f}")
        print(f"   ‚Ä¢ R√µ r√†ng: {components['clarity']['mean']:.3f}")
        print(f"   ‚Ä¢ ƒê·∫ßy ƒë·ªß: {components['completeness']['mean']:.3f}")
        print(f"   ‚Ä¢ Gi·ªçng ƒëi·ªáu: {components['tone_appropriateness']['mean']:.3f}")
        print(f"   ‚Ä¢ Ch·∫•t l∆∞·ª£ng ngu·ªìn: {components['source_quality']['mean']:.3f}")
        print(f"   ‚Ä¢ C·∫•u tr√∫c: {components['structure']['mean']:.3f}")
        
        print(f"\nüìà Ph√¢n t√≠ch theo ƒë·ªô ph·ª©c t·∫°p:")
        for complexity, analysis in metrics['complexity_analysis'].items():
            print(f"   ‚Ä¢ {complexity.title()}: {analysis['avg_score']:.3f} (n={analysis['count']})")
        
        print(f"\nüë• Ph√¢n t√≠ch theo lo·∫°i ng∆∞·ªùi d√πng:")
        for user_type, analysis in metrics['user_type_analysis'].items():
            print(f"   ‚Ä¢ {user_type.title()}: {analysis['avg_score']:.3f} (n={analysis['count']})")

async def main():
    """Ch·∫°y response quality testing"""
    async with ResponseQualityTester() as tester:
        print("üìù B·∫ÆT ƒê·∫¶U TH·ª¨ NGHI·ªÜM CH·∫§T L∆Ø·ª¢NG PH·∫¢N H·ªíI H·ªÜ TH·ªêNG TR·ª¢ L√ù ·∫¢O AN TO√ÄN TH√îNG TIN")
        print("="*80)
        
        # Ch·∫°y t·∫•t c·∫£ quality test cases
        results = await tester.run_all_quality_tests()
        
        # T√≠nh to√°n metrics
        metrics = tester.calculate_quality_metrics(results)
        
        # In k·∫øt qu·∫£
        tester.print_summary(metrics)
        
        # L∆∞u k·∫øt qu·∫£
        tester.save_results(metrics, results)
        
        print(f"\n‚úÖ HO√ÄN TH√ÄNH TH·ª¨ NGHI·ªÜM CH·∫§T L∆Ø·ª¢NG PH·∫¢N H·ªíI")
        print("="*80)

if __name__ == "__main__":
    asyncio.run(main())
