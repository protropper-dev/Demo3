#!/usr/bin/env python3
"""
Comprehensive Testing Script cho Há»‡ thá»‘ng Trá»£ lÃ½ áº¢o An toÃ n ThÃ´ng tin
Cháº¡y táº¥t cáº£ cÃ¡c loáº¡i thá»­ nghiá»‡m: Performance, Accuracy, Response Quality
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, Any, List
import logging
import os
import sys

# Import cÃ¡c module test
from test_performance import PerformanceTester
from test_accuracy import AccuracyTester
from test_response_quality import ResponseQualityTester

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_test_results.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ComprehensiveTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.start_time = None
        self.end_time = None
        self.all_results = {}
        
    async def run_performance_tests(self) -> Dict[str, Any]:
        """Cháº¡y cÃ¡c test hiá»‡u suáº¥t"""
        logger.info("ğŸš€ Báº¯t Ä‘áº§u Performance Testing...")
        
        async with PerformanceTester(self.base_url) as tester:
            # Test 1: Single Request
            single_result = await tester.single_request_test("An toÃ n thÃ´ng tin lÃ  gÃ¬?")
            single_metrics = tester.calculate_performance_metrics([single_result])
            
            # Test 2: Concurrent Requests
            concurrent_results = await tester.concurrent_requests_test(10)
            concurrent_metrics = tester.calculate_performance_metrics(concurrent_results)
            
            # Test 3: Load Test (ngáº¯n hÆ¡n cho comprehensive test)
            load_results = await tester.load_test(duration_seconds=20, requests_per_second=2)
            load_metrics = tester.calculate_performance_metrics(load_results)
            
            # Test 4: Stress Test (nháº¹ hÆ¡n)
            stress_results = await tester.stress_test(max_concurrent=20)
            stress_metrics = tester.calculate_performance_metrics(stress_results)
            
            performance_results = {
                "single_request": {
                    "metrics": single_metrics,
                    "results": [single_result]
                },
                "concurrent_requests": {
                    "metrics": concurrent_metrics,
                    "results": concurrent_results
                },
                "load_test": {
                    "metrics": load_metrics,
                    "results": load_results
                },
                "stress_test": {
                    "metrics": stress_metrics,
                    "results": stress_results
                }
            }
            
            logger.info("âœ… Performance Testing hoÃ n thÃ nh")
            return performance_results
    
    async def run_accuracy_tests(self) -> Dict[str, Any]:
        """Cháº¡y cÃ¡c test Ä‘á»™ chÃ­nh xÃ¡c"""
        logger.info("ğŸ¯ Báº¯t Ä‘áº§u Accuracy Testing...")
        
        async with AccuracyTester(self.base_url) as tester:
            # Cháº¡y táº¥t cáº£ accuracy test cases
            results = await tester.run_all_tests()
            metrics = tester.calculate_overall_metrics(results)
            
            accuracy_results = {
                "metrics": metrics,
                "results": results
            }
            
            logger.info("âœ… Accuracy Testing hoÃ n thÃ nh")
            return accuracy_results
    
    async def run_response_quality_tests(self) -> Dict[str, Any]:
        """Cháº¡y cÃ¡c test cháº¥t lÆ°á»£ng pháº£n há»“i"""
        logger.info("ğŸ“ Báº¯t Ä‘áº§u Response Quality Testing...")
        
        async with ResponseQualityTester(self.base_url) as tester:
            # Cháº¡y táº¥t cáº£ quality test cases
            results = await tester.run_all_quality_tests()
            metrics = tester.calculate_quality_metrics(results)
            
            quality_results = {
                "metrics": metrics,
                "results": results
            }
            
            logger.info("âœ… Response Quality Testing hoÃ n thÃ nh")
            return quality_results
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """Cháº¡y táº¥t cáº£ cÃ¡c loáº¡i test"""
        self.start_time = datetime.now()
        logger.info("ğŸ”¬ Báº®T Äáº¦U THá»¬ NGHIá»†M TOÃ€N DIá»†N Há»† THá»NG TRá»¢ LÃ áº¢O AN TOÃ€N THÃ”NG TIN")
        logger.info("="*80)
        
        all_results = {}
        
        try:
            # 1. Performance Testing
            logger.info("\nğŸ“Š PHáº¦N 1: THá»¬ NGHIá»†M HIá»†U SUáº¤T")
            logger.info("-" * 50)
            all_results["performance"] = await self.run_performance_tests()
            
            # 2. Accuracy Testing
            logger.info("\nğŸ¯ PHáº¦N 2: THá»¬ NGHIá»†M Äá»˜ CHÃNH XÃC")
            logger.info("-" * 50)
            all_results["accuracy"] = await self.run_accuracy_tests()
            
            # 3. Response Quality Testing
            logger.info("\nğŸ“ PHáº¦N 3: THá»¬ NGHIá»†M CHáº¤T LÆ¯á»¢NG PHáº¢N Há»’I")
            logger.info("-" * 50)
            all_results["response_quality"] = await self.run_response_quality_tests()
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh test: {e}")
            all_results["error"] = str(e)
        
        self.end_time = datetime.now()
        all_results["test_summary"] = self._generate_test_summary(all_results)
        
        return all_results
    
    def _generate_test_summary(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """Táº¡o tÃ³m táº¯t tá»•ng thá»ƒ cá»§a táº¥t cáº£ cÃ¡c test"""
        summary = {
            "test_start_time": self.start_time.isoformat() if self.start_time else None,
            "test_end_time": self.end_time.isoformat() if self.end_time else None,
            "total_duration_minutes": None,
            "overall_status": "completed",
            "test_categories": {},
            "overall_score": 0.0,
            "recommendations": []
        }
        
        if self.start_time and self.end_time:
            duration = (self.end_time - self.start_time).total_seconds()
            summary["total_duration_minutes"] = round(duration / 60, 2)
        
        # TÃ­nh Ä‘iá»ƒm tá»•ng thá»ƒ
        scores = []
        
        # Performance scores
        if "performance" in all_results:
            perf = all_results["performance"]
            perf_score = self._calculate_performance_score(perf)
            summary["test_categories"]["performance"] = {
                "status": "completed",
                "score": perf_score,
                "details": {
                    "avg_response_time_ms": self._get_avg_response_time(perf),
                    "success_rate": self._get_success_rate(perf),
                    "throughput_rps": self._get_throughput(perf)
                }
            }
            scores.append(perf_score)
        
        # Accuracy scores
        if "accuracy" in all_results:
            acc = all_results["accuracy"]
            acc_score = acc["metrics"].get("overall_scores", {}).get("mean", 0.0)
            summary["test_categories"]["accuracy"] = {
                "status": "completed",
                "score": acc_score,
                "details": {
                    "success_rate": acc["metrics"].get("success_rate", 0.0),
                    "keyword_coverage": acc["metrics"].get("component_scores", {}).get("keyword_coverage", {}).get("mean", 0.0),
                    "technical_accuracy": acc["metrics"].get("component_scores", {}).get("technical_accuracy", {}).get("mean", 0.0)
                }
            }
            scores.append(acc_score)
        
        # Response Quality scores
        if "response_quality" in all_results:
            qual = all_results["response_quality"]
            qual_score = qual["metrics"].get("overall_quality_scores", {}).get("mean", 0.0)
            summary["test_categories"]["response_quality"] = {
                "status": "completed",
                "score": qual_score,
                "details": {
                    "success_rate": qual["metrics"].get("success_rate", 0.0),
                    "coherence": qual["metrics"].get("component_scores", {}).get("coherence", {}).get("mean", 0.0),
                    "helpfulness": qual["metrics"].get("component_scores", {}).get("helpfulness", {}).get("mean", 0.0)
                }
            }
            scores.append(qual_score)
        
        # TÃ­nh Ä‘iá»ƒm tá»•ng thá»ƒ
        if scores:
            summary["overall_score"] = sum(scores) / len(scores)
        
        # Táº¡o khuyáº¿n nghá»‹
        summary["recommendations"] = self._generate_recommendations(summary)
        
        return summary
    
    def _calculate_performance_score(self, performance_results: Dict[str, Any]) -> float:
        """TÃ­nh Ä‘iá»ƒm hiá»‡u suáº¥t tá»•ng thá»ƒ"""
        scores = []
        
        # Äiá»ƒm dá»±a trÃªn response time (cÃ ng nhanh cÃ ng tá»‘t)
        for test_type in ["single_request", "concurrent_requests", "load_test"]:
            if test_type in performance_results:
                avg_response_time = performance_results[test_type]["metrics"].get("response_time", {}).get("mean", 0)
                # Chuyá»ƒn Ä‘á»•i response time thÃ nh Ä‘iá»ƒm (dÆ°á»›i 2s = 1.0, trÃªn 10s = 0.0)
                response_score = max(0, 1 - (avg_response_time - 2000) / 8000)
                scores.append(response_score)
        
        # Äiá»ƒm dá»±a trÃªn success rate
        for test_type in performance_results:
            success_rate = performance_results[test_type]["metrics"].get("success_rate", 0)
            scores.append(success_rate / 100)
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _get_avg_response_time(self, performance_results: Dict[str, Any]) -> float:
        """Láº¥y thá»i gian pháº£n há»“i trung bÃ¬nh"""
        times = []
        for test_type in performance_results:
            response_time = performance_results[test_type]["metrics"].get("response_time", {}).get("mean", 0)
            if response_time > 0:
                times.append(response_time)
        return sum(times) / len(times) if times else 0.0
    
    def _get_success_rate(self, performance_results: Dict[str, Any]) -> float:
        """Láº¥y tá»· lá»‡ thÃ nh cÃ´ng trung bÃ¬nh"""
        rates = []
        for test_type in performance_results:
            success_rate = performance_results[test_type]["metrics"].get("success_rate", 0)
            rates.append(success_rate)
        return sum(rates) / len(rates) if rates else 0.0
    
    def _get_throughput(self, performance_results: Dict[str, Any]) -> float:
        """Láº¥y throughput trung bÃ¬nh"""
        throughputs = []
        for test_type in performance_results:
            throughput = performance_results[test_type]["metrics"].get("throughput", {}).get("requests_per_second", 0)
            if throughput > 0:
                throughputs.append(throughput)
        return sum(throughputs) / len(throughputs) if throughputs else 0.0
    
    def _generate_recommendations(self, summary: Dict[str, Any]) -> List[str]:
        """Táº¡o cÃ¡c khuyáº¿n nghá»‹ cáº£i thiá»‡n"""
        recommendations = []
        
        overall_score = summary.get("overall_score", 0.0)
        
        if overall_score < 0.6:
            recommendations.append("âš ï¸ Äiá»ƒm tá»•ng thá»ƒ tháº¥p - Cáº§n cáº£i thiá»‡n toÃ n diá»‡n há»‡ thá»‘ng")
        elif overall_score < 0.8:
            recommendations.append("âœ… Äiá»ƒm tá»•ng thá»ƒ khÃ¡ - CÃ³ thá»ƒ cáº£i thiá»‡n má»™t sá»‘ khÃ­a cáº¡nh")
        else:
            recommendations.append("ğŸ‰ Äiá»ƒm tá»•ng thá»ƒ tá»‘t - Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh")
        
        # Khuyáº¿n nghá»‹ cá»¥ thá»ƒ cho tá»«ng loáº¡i test
        categories = summary.get("test_categories", {})
        
        if "performance" in categories:
            perf_details = categories["performance"]["details"]
            if perf_details["avg_response_time_ms"] > 5000:
                recommendations.append("ğŸš€ Cáº§n tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t - thá»i gian pháº£n há»“i quÃ¡ cháº­m")
            if perf_details["success_rate"] < 95:
                recommendations.append("ğŸ”§ Cáº§n cáº£i thiá»‡n Ä‘á»™ á»•n Ä‘á»‹nh - tá»· lá»‡ thÃ nh cÃ´ng tháº¥p")
        
        if "accuracy" in categories:
            acc_details = categories["accuracy"]["details"]
            if acc_details["keyword_coverage"] < 0.7:
                recommendations.append("ğŸ¯ Cáº§n cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c - tá»· lá»‡ bao phá»§ tá»« khÃ³a tháº¥p")
            if acc_details["technical_accuracy"] < 0.6:
                recommendations.append("ğŸ“š Cáº§n cáº£i thiá»‡n kiáº¿n thá»©c ká»¹ thuáº­t trong knowledge base")
        
        if "response_quality" in categories:
            qual_details = categories["response_quality"]["details"]
            if qual_details["coherence"] < 0.7:
                recommendations.append("ğŸ“ Cáº§n cáº£i thiá»‡n tÃ­nh máº¡ch láº¡c cá»§a cÃ¢u tráº£ lá»i")
            if qual_details["helpfulness"] < 0.6:
                recommendations.append("ğŸ’¡ Cáº§n cáº£i thiá»‡n tÃ­nh há»¯u Ã­ch cá»§a cÃ¢u tráº£ lá»i")
        
        return recommendations
    
    def save_comprehensive_results(self, all_results: Dict[str, Any]):
        """LÆ°u káº¿t quáº£ tá»•ng há»£p"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"comprehensive_test_results_{timestamp}.json"
        
        output = {
            "test_type": "comprehensive_testing",
            "timestamp": timestamp,
            "system_info": {
                "base_url": self.base_url,
                "test_duration_minutes": all_results.get("test_summary", {}).get("total_duration_minutes", 0)
            },
            "results": all_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š Káº¿t quáº£ comprehensive test Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {filename}")
        return filename
    
    def print_comprehensive_summary(self, all_results: Dict[str, Any]):
        """In tÃ³m táº¯t tá»•ng há»£p"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š BÃO CÃO Tá»”NG Há»¢P THá»¬ NGHIá»†M Há»† THá»NG TRá»¢ LÃ áº¢O AN TOÃ€N THÃ”NG TIN")
        print(f"{'='*80}")
        
        summary = all_results.get("test_summary", {})
        
        # ThÃ´ng tin chung
        print(f"\nâ±ï¸  ThÃ´ng tin chung:")
        print(f"   â€¢ Thá»i gian báº¯t Ä‘áº§u: {summary.get('test_start_time', 'N/A')}")
        print(f"   â€¢ Thá»i gian káº¿t thÃºc: {summary.get('test_end_time', 'N/A')}")
        print(f"   â€¢ Tá»•ng thá»i gian: {summary.get('total_duration_minutes', 0):.2f} phÃºt")
        print(f"   â€¢ Tráº¡ng thÃ¡i: {summary.get('overall_status', 'unknown').upper()}")
        
        # Äiá»ƒm tá»•ng thá»ƒ
        overall_score = summary.get("overall_score", 0.0)
        print(f"\nğŸ† ÄIá»‚M Tá»”NG THá»‚: {overall_score:.3f}/1.0")
        
        if overall_score >= 0.8:
            print("   ğŸ‰ XUáº¤T Sáº®C - Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng ráº¥t tá»‘t")
        elif overall_score >= 0.6:
            print("   âœ… Tá»T - Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh")
        elif overall_score >= 0.4:
            print("   âš ï¸  TRUNG BÃŒNH - Cáº§n cáº£i thiá»‡n")
        else:
            print("   âŒ KÃ‰M - Cáº§n cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ")
        
        # Chi tiáº¿t tá»«ng loáº¡i test
        categories = summary.get("test_categories", {})
        
        print(f"\nğŸ“Š CHI TIáº¾T Tá»ªNG LOáº I THá»¬ NGHIá»†M:")
        print("-" * 60)
        
        if "performance" in categories:
            perf = categories["performance"]
            print(f"\nğŸš€ HIá»†U SUáº¤T:")
            print(f"   â€¢ Äiá»ƒm: {perf['score']:.3f}/1.0")
            details = perf["details"]
            print(f"   â€¢ Thá»i gian pháº£n há»“i TB: {details['avg_response_time_ms']:.0f}ms")
            print(f"   â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {details['success_rate']:.1f}%")
            print(f"   â€¢ Throughput: {details['throughput_rps']:.2f} req/s")
        
        if "accuracy" in categories:
            acc = categories["accuracy"]
            print(f"\nğŸ¯ Äá»˜ CHÃNH XÃC:")
            print(f"   â€¢ Äiá»ƒm: {acc['score']:.3f}/1.0")
            details = acc["details"]
            print(f"   â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {details['success_rate']:.1f}%")
            print(f"   â€¢ Bao phá»§ tá»« khÃ³a: {details['keyword_coverage']:.3f}")
            print(f"   â€¢ ChÃ­nh xÃ¡c ká»¹ thuáº­t: {details['technical_accuracy']:.3f}")
        
        if "response_quality" in categories:
            qual = categories["response_quality"]
            print(f"\nğŸ“ CHáº¤T LÆ¯á»¢NG PHáº¢N Há»’I:")
            print(f"   â€¢ Äiá»ƒm: {qual['score']:.3f}/1.0")
            details = qual["details"]
            print(f"   â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {details['success_rate']:.1f}%")
            print(f"   â€¢ TÃ­nh máº¡ch láº¡c: {details['coherence']:.3f}")
            print(f"   â€¢ TÃ­nh há»¯u Ã­ch: {details['helpfulness']:.3f}")
        
        # Khuyáº¿n nghá»‹
        recommendations = summary.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ KHUYáº¾N NGHá»Š Cáº¢I THIá»†N:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print(f"\n{'='*80}")
        print("âœ… HOÃ€N THÃ€NH THá»¬ NGHIá»†M TOÃ€N DIá»†N")
        print("="*80)

async def main():
    """Cháº¡y comprehensive testing"""
    # Kiá»ƒm tra xem server cÃ³ Ä‘ang cháº¡y khÃ´ng
    base_url = "http://localhost:8000"
    
    print("ğŸ” Kiá»ƒm tra káº¿t ná»‘i Ä‘áº¿n server...")
    try:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{base_url}/health", timeout=5) as response:
                if response.status == 200:
                    print("âœ… Server Ä‘ang hoáº¡t Ä‘á»™ng")
                else:
                    print(f"âš ï¸ Server tráº£ vá» status {response.status}")
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n server: {e}")
        print("ğŸ’¡ HÃ£y Ä‘áº£m báº£o backend server Ä‘ang cháº¡y trÃªn http://localhost:8000")
        return
    
    # Cháº¡y comprehensive test
    tester = ComprehensiveTester(base_url)
    
    try:
        all_results = await tester.run_all_tests()
        
        # In káº¿t quáº£
        tester.print_comprehensive_summary(all_results)
        
        # LÆ°u káº¿t quáº£
        filename = tester.save_comprehensive_results(all_results)
        
        print(f"\nğŸ“ Táº¥t cáº£ káº¿t quáº£ chi tiáº¿t Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong: {filename}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Test bá»‹ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
    except Exception as e:
        logger.error(f"âŒ Lá»—i trong comprehensive test: {e}")
        print(f"âŒ CÃ³ lá»—i xáº£y ra: {e}")

if __name__ == "__main__":
    asyncio.run(main())
