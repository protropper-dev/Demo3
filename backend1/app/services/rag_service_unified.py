#!/usr/bin/env python3
"""
RAG Service Unified - API th·ªëng nh·∫•t cho query/response
H·ª£p nh·∫•t t·∫•t c·∫£ c√°c ch·ª©c nƒÉng RAG th√†nh m·ªôt service m·∫°nh m·∫Ω
"""

import os
import logging
import pickle
import faiss
import numpy as np
from typing import List, Dict, Any, Optional, Union
from transformers import AutoTokenizer, AutoModel
import torch
import time
from datetime import datetime
from app.services.llm_service import LLMService

logger = logging.getLogger(__name__)

class RAGServiceUnified:
    """RAG Service th·ªëng nh·∫•t - x·ª≠ l√Ω t·∫•t c·∫£ query/response"""
    
    def __init__(self):
        self.is_initialized = False
        self.tokenizer = None
        self.model = None
        self.device = None
        self.faiss_index = None
        self.documents_data = None
        self.chunks_metadata = []
        self.total_chunks = 0
        self.total_documents = 0
        self.initialization_time = None
        
        # LLM Service cho text generation
        self.llm_service = None
        
        # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
        self.default_top_k = 5
        self.default_similarity_threshold = 0.3
        self.max_answer_length = 2000
        self.max_context_length = 1500
        self.use_llm_generation = True  # T·∫°m th·ªùi disable ƒë·ªÉ test template - set True ƒë·ªÉ enable LLM
        
    async def initialize(self):
        """Kh·ªüi t·∫°o service v·ªõi d·ªØ li·ªáu c√≥ s·∫µn"""
        try:
            start_time = time.time()
            logger.info("üöÄ ƒêang kh·ªüi t·∫°o RAG Service Unified...")
            
            # 1. Load embedding model
            model_path = "D:/Vian/MODELS/multilingual_e5_large"
            logger.info(f"üì• Loading embedding model: {model_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModel.from_pretrained(model_path)
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model.to(self.device)
            
            logger.info(f"‚úÖ Embedding model loaded on {self.device}")
            
            # 2. Initialize LLM Service
            if self.use_llm_generation:
                try:
                    llm_model_path = "D:/Vian/MODELS/vinallama-2.7b-chat"
                    logger.info(f"üì• Initializing LLM service: {llm_model_path}")
                    
                    self.llm_service = LLMService(llm_model_path)
                    self.llm_service.load_model()
                    
                    logger.info("‚úÖ LLM service initialized successfully")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è LLM service failed to load: {e}")
                    logger.warning("Will use template-based responses instead")
                    self.use_llm_generation = False
            
            # 3. Load d·ªØ li·ªáu
            data_dir = "D:/Vian/Demo3/backend1/data"
            faiss_path = os.path.join(data_dir, "all_faiss.index")
            pickle_path = os.path.join(data_dir, "all_embeddings.pkl")
            
            logger.info(f"üì• Loading FAISS index: {faiss_path}")
            self.faiss_index = faiss.read_index(faiss_path)
            
            logger.info(f"üì• Loading documents data: {pickle_path}")
            with open(pickle_path, 'rb') as f:
                self.documents_data = pickle.load(f)
            
            # 3. T·∫°o chunks metadata v·ªõi category mapping
            logger.info("üîÑ Processing chunks metadata...")
            self.chunks_metadata = []
            
            for doc_idx, doc in enumerate(self.documents_data):
                pdf_name = doc.get('pdf_name', 'Unknown')
                chunks = doc.get('chunks', [])
                
                # X√°c ƒë·ªãnh category d·ª±a tr√™n pdf_name
                category = self._determine_category(pdf_name)
                
                for chunk_idx, chunk in enumerate(chunks):
                    self.chunks_metadata.append({
                        'doc_idx': doc_idx,
                        'chunk_idx': chunk_idx,
                        'pdf_name': pdf_name,
                        'content': chunk,
                        'category': category,
                        'content_length': len(chunk)
                    })
            
            self.total_chunks = len(self.chunks_metadata)
            self.total_documents = len(self.documents_data)
            
            end_time = time.time()
            self.initialization_time = end_time - start_time
            
            self.is_initialized = True
            
            logger.info(f"‚úÖ RAG Service Unified initialized successfully!")
            logger.info(f"üìä Stats: {self.total_documents} documents, {self.total_chunks} chunks")
            logger.info(f"‚è±Ô∏è Initialization time: {self.initialization_time:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o RAG Service Unified: {e}")
            raise
    
    def _extract_main_topic(self, question_lower: str) -> str:
        """Tr√≠ch xu·∫•t ch·ªß ƒë·ªÅ ch√≠nh t·ª´ c√¢u h·ªèi"""
        # Mapping keywords to topics
        topic_keywords = {
            't∆∞·ªùng l·ª≠a': ['t∆∞·ªùng l·ª≠a', 'firewall', 'waf', 'ids', 'ips'],
            'ddos': ['ddos', 'dos', 'denial of service', 't·ª´ ch·ªëi d·ªãch v·ª•'],
            'malware': ['malware', 'virus', 'trojan', 'ransomware', 'm√£ ƒë·ªôc'],
            'phishing': ['phishing', 'l·ª´a ƒë·∫£o', 'gi·∫£ m·∫°o'],
            'm·∫≠t kh·∫©u': ['m·∫≠t kh·∫©u', 'password', 'authentication'],
            'm√£ h√≥a': ['m√£ h√≥a', 'encryption', 'cryptography'],
            'iso 27001': ['iso 27001', 'iso27001'],
            'nist': ['nist', 'cybersecurity framework'],
            'an to√†n th√¥ng tin': ['an to√†n th√¥ng tin', 'information security', 'b·∫£o m·∫≠t']
        }
        
        # T√¨m topic ph√π h·ª£p nh·∫•t
        for topic, keywords in topic_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                return topic
        
        return 'general'
    
    def _generate_dynamic_title(self, main_topic: str, answer_style: str) -> str:
        """T·∫°o ti√™u ƒë·ªÅ ƒë·ªông d·ª±a tr√™n topic"""
        if answer_style == 'definition':
            if main_topic == 't∆∞·ªùng l·ª≠a':
                return "ƒê·ªãnh nghƒ©a T∆∞·ªùng l·ª≠a (Firewall)"
            elif main_topic == 'ddos':
                return "T·∫•n c√¥ng DDoS l√† g√¨"
            elif main_topic == 'malware':
                return "ƒê·ªãnh nghƒ©a Malware v√† M√£ ƒë·ªôc"
            elif main_topic == 'phishing':
                return "T·∫•n c√¥ng Phishing l√† g√¨"
            elif main_topic == 'm·∫≠t kh·∫©u':
                return "ƒê·ªãnh nghƒ©a v·ªÅ M·∫≠t kh·∫©u v√† X√°c th·ª±c"
            elif main_topic == 'm√£ h√≥a':
                return "M√£ h√≥a (Encryption) l√† g√¨"
            elif main_topic == 'iso 27001':
                return "Ti√™u chu·∫©n ISO 27001"
            elif main_topic == 'nist':
                return "NIST Cybersecurity Framework"
            else:
                return "ƒê·ªãnh nghƒ©a An to√†n th√¥ng tin"
        elif answer_style == 'regulation':
            return f"Quy ƒë·ªãnh ph√°p l√Ω v·ªÅ {main_topic.title()}"
        elif answer_style == 'standard':
            return f"Ti√™u chu·∫©n v·ªÅ {main_topic.title()}"
        elif answer_style == 'attack':
            return f"Th√¥ng tin v·ªÅ {main_topic.title()}"
        else:
            return f"Th√¥ng tin v·ªÅ {main_topic.title()}"
    
    def _generate_dynamic_conclusion(self, main_topic: str, answer_style: str) -> str:
        """T·∫°o k·∫øt lu·∫≠n ƒë·ªông d·ª±a tr√™n topic"""
        conclusions = {
            't∆∞·ªùng l·ª≠a': "T∆∞·ªùng l·ª≠a l√† h·ªá th·ªëng b·∫£o m·∫≠t m·∫°ng quan tr·ªçng, gi√∫p ki·ªÉm so√°t v√† l·ªçc l∆∞u l∆∞·ª£ng m·∫°ng, b·∫£o v·ªá h·ªá th·ªëng kh·ªèi c√°c truy c·∫≠p tr√°i ph√©p.",
            'ddos': "T·∫•n c√¥ng DDoS l√† m·ªôt trong nh·ªØng m·ªëi ƒëe d·ªça nghi√™m tr·ªçng nh·∫•t ƒë·ªëi v·ªõi h·ªá th·ªëng m·∫°ng, c√≥ th·ªÉ g√¢y gi√°n ƒëo·∫°n d·ªãch v·ª• v√† thi·ªát h·∫°i kinh t·∫ø l·ªõn.",
            'malware': "Malware l√† m·ªëi ƒëe d·ªça th∆∞·ªùng xuy√™n trong kh√¥ng gian m·∫°ng, c·∫ßn c√≥ bi·ªán ph√°p ph√≤ng ch·ªëng ƒëa l·ªõp ƒë·ªÉ b·∫£o v·ªá h·ªá th·ªëng.",
            'phishing': "Phishing l√† h√¨nh th·ª©c t·∫•n c√¥ng k·ªπ thu·∫≠t x√£ h·ªôi nguy hi·ªÉm, c·∫ßn n√¢ng cao nh·∫≠n th·ª©c ng∆∞·ªùi d√πng ƒë·ªÉ ph√≤ng ch·ªëng hi·ªáu qu·∫£.",
            'm·∫≠t kh·∫©u': "M·∫≠t kh·∫©u m·∫°nh v√† x√°c th·ª±c ƒëa y·∫øu t·ªë l√† n·ªÅn t·∫£ng c·ªßa b·∫£o m·∫≠t h·ªá th·ªëng th√¥ng tin.",
            'm√£ h√≥a': "M√£ h√≥a l√† c√¥ng ngh·ªá c·ªët l√µi ƒë·ªÉ b·∫£o v·ªá t√≠nh b·∫£o m·∫≠t v√† to√†n v·∫πn c·ªßa d·ªØ li·ªáu.",
            'iso 27001': "ISO 27001 l√† ti√™u chu·∫©n qu·ªëc t·∫ø quan tr·ªçng cho h·ªá th·ªëng qu·∫£n l√Ω an to√†n th√¥ng tin.",
            'nist': "NIST Framework cung c·∫•p h∆∞·ªõng d·∫´n to√†n di·ªán ƒë·ªÉ qu·∫£n l√Ω r·ªßi ro cybersecurity.",
            'an to√†n th√¥ng tin': "An to√†n th√¥ng tin l√† lƒ©nh v·ª±c b·∫£o v·ªá th√¥ng tin v√† h·ªá th·ªëng th√¥ng tin kh·ªèi c√°c m·ªëi ƒëe d·ªça, ƒë·∫£m b·∫£o t√≠nh b·∫£o m·∫≠t (Confidentiality), to√†n v·∫πn (Integrity) v√† s·∫µn s√†ng (Availability) c·ªßa d·ªØ li·ªáu."
        }
        
        return conclusions.get(main_topic, "ƒê√¢y l√† m·ªôt kh√≠a c·∫°nh quan tr·ªçng c·ªßa an to√†n th√¥ng tin c·∫ßn ƒë∆∞·ª£c hi·ªÉu r√µ v√† tri·ªÉn khai ƒë√∫ng c√°ch.")
    
    def _generate_llm_answer(self, question: str, search_results: List[Dict]) -> Dict[str, Any]:
        """T·∫°o c√¢u tr·∫£ l·ªùi s·ª≠ d·ª•ng LLM v·ªõi RAG context"""
        try:
            logger.info(f"ü§ñ Attempting LLM generation for: {question[:50]}...")
            
            # Ki·ªÉm tra LLM service
            if not self.llm_service:
                logger.warning("‚ùå LLM service not available, using template")
                return self._generate_template_answer(question, search_results)
            
            # Chu·∫©n b·ªã context t·ª´ search results
            context_docs = []
            for result in search_results[:3]:  # Top 3 results
                context_docs.append({
                    'content': result['content'][:800],  # Gi·ªõi h·∫°n length ƒë·ªÉ tr√°nh token limit
                    'metadata': {
                        'filename': result['pdf_name'],
                        'category': result['category'],
                        'similarity': result['similarity']
                    }
                })
            
            logger.info(f"üìñ Prepared {len(context_docs)} context docs for LLM")
            
            # S·ª≠ d·ª•ng LLM service ƒë·ªÉ generate response
            llm_response = self.llm_service.generate_response(
                query=question,
                context_docs=context_docs,
                max_new_tokens=256  # Gi·∫£m ƒë·ªÉ tr√°nh timeout
            )
            
            # Ki·ªÉm tra response quality
            if not llm_response or len(llm_response.strip()) < 20:
                logger.warning("‚ùå LLM response too short, using template fallback")
                return self._generate_template_answer(question, search_results)
            
            if "kh√¥ng th·ªÉ" in llm_response or "th·ª≠ l·∫°i" in llm_response:
                logger.warning("‚ùå LLM returned error message, using template fallback")
                return self._generate_template_answer(question, search_results)
            
            # T√≠nh confidence d·ª±a tr√™n search results quality
            avg_score = sum(r['score'] for r in search_results[:3]) / min(3, len(search_results))
            confidence = min(avg_score / 200.0, 1.0)
            
            logger.info(f"‚úÖ LLM generation successful: {len(llm_response)} chars")
            
            return {
                'answer': llm_response,
                'confidence': confidence,
                'method': 'llm_generation',
                'sources_used': len(search_results[:3])
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói LLM generation: {e}")
            logger.info("üîÑ Falling back to template generation")
            # Fallback to template
            return self._generate_template_answer(question, search_results)
    
    def _generate_template_answer(self, question: str, search_results: List[Dict]) -> Dict[str, Any]:
        """T·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng template (fallback)"""
        try:
            # 1. Ph√¢n t√≠ch c√¢u h·ªèi ƒë·ªÉ x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ v√† style
            question_lower = question.lower()
            
            # X√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh t·ª´ c√¢u h·ªèi
            main_topic = self._extract_main_topic(question_lower)
            
            # X√°c ƒë·ªãnh style d·ª±a tr√™n c√¢u h·ªèi
            is_definition = any(word in question_lower for word in ['l√† g√¨', 'what is', 'ƒë·ªãnh nghƒ©a', 'kh√°i ni·ªám'])
            is_regulation = any(word in question_lower for word in ['lu·∫≠t', 'quy ƒë·ªãnh', 'regulation', 'law'])
            is_standard = any(word in question_lower for word in ['iso', 'nist', 'ti√™u chu·∫©n', 'standard'])
            is_attack = any(word in question_lower for word in ['t·∫•n c√¥ng', 'attack', 'hack', 'malware', 'virus'])
            
            # 2. L·∫•y top results d·ª±a tr√™n lo·∫°i c√¢u h·ªèi
            if is_definition:
                top_results = search_results[:3]
                answer_style = 'definition'
            elif is_regulation:
                top_results = search_results[:4]
                answer_style = 'regulation'
            elif is_standard:
                top_results = search_results[:3]
                answer_style = 'standard'
            elif is_attack:
                top_results = search_results[:3]
                answer_style = 'attack'
            else:
                top_results = search_results[:3]
                answer_style = 'general'
            
            # 3. T·∫°o ti√™u ƒë·ªÅ ƒë·ªông d·ª±a tr√™n topic v√† style
            answer_parts = []
            title = self._generate_dynamic_title(main_topic, answer_style)
            answer_parts.append(f"**{title}:**")
            answer_parts.append("")
            
            # 4. Th√™m n·ªôi dung t·ª´ t·ª´ng source
            for i, result in enumerate(top_results, 1):
                pdf_name = result['pdf_name']
                content = result['content']
                category = result['category']
                
                # L√†m s·∫°ch t√™n file ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n
                display_name = self._clean_filename(pdf_name)
                
                # T√¨m c√¢u quan tr·ªçng nh·∫•t
                important_sentences = self._extract_important_sentences(content, question_lower)
                
                if important_sentences:
                    answer_parts.append(f"**{i}. Theo {display_name}:**")
                    for sentence in important_sentences[:2]:  # T·ªëi ƒëa 2 c√¢u
                        cleaned_sentence = self._clean_sentence(sentence.strip())
                        if cleaned_sentence and len(cleaned_sentence) > 20:
                            answer_parts.append(f"‚Ä¢ {cleaned_sentence}")
                    answer_parts.append("")
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y c√¢u c·ª• th·ªÉ, t·∫°o summary t·ª´ content
                    summary = self._create_content_summary(content, question_lower)
                    if summary:
                        answer_parts.append(f"**{i}. Theo {display_name}:**")
                        answer_parts.append(f"‚Ä¢ {summary}")
                        answer_parts.append("")
            
            # 5. Th√™m t·ªïng k·∫øt ƒë·ªông d·ª±a tr√™n topic
            answer_parts.append("---")
            conclusion = self._generate_dynamic_conclusion(main_topic, answer_style)
            answer_parts.append(f"**T√≥m l·∫°i:** {conclusion}")
            
            answer_text = "\n".join(answer_parts)
            
            # 6. T√≠nh confidence d·ª±a tr√™n quality c·ªßa results
            avg_score = sum(r['score'] for r in top_results) / len(top_results)
            confidence = min(avg_score / 200.0, 1.0)  # Normalize to 0-1
            
            return {
                'answer': answer_text,
                'confidence': confidence,
                'method': answer_style + '_template',
                'sources_used': len(top_results)
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói template generation: {e}")
            return {
                'answer': "Xin l·ªói, c√≥ l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i.",
                'confidence': 0.0,
                'method': 'error'
            }
    
    def _determine_category(self, pdf_name: str) -> str:
        """X√°c ƒë·ªãnh category d·ª±a tr√™n t√™n file"""
        pdf_lower = pdf_name.lower()
        
        if any(keyword in pdf_lower for keyword in ['luat', 'nghi_dinh', 'quyet_dinh', 'thong_tu', 'tcvn']):
            return 'luat'
        elif any(keyword in pdf_lower for keyword in ['nist', 'iso', 'cybersecurity', 'framework']):
            return 'english'
        else:
            return 'vietnamese'
    
    def encode_text(self, text: str) -> np.ndarray:
        """Encode text th√†nh embedding vector"""
        try:
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                return embeddings.cpu().numpy()
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói encode text: {e}")
            raise
    
    def search_relevant_chunks(self, 
                             question: str, 
                             top_k: int = None,
                             filter_category: Optional[str] = None,
                             similarity_threshold: float = None) -> List[Dict]:
        """T√¨m ki·∫øm chunks li√™n quan v·ªõi filtering n√¢ng cao"""
        try:
            if not self.is_initialized:
                raise RuntimeError("Service ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            
            if top_k is None:
                top_k = self.default_top_k
            if similarity_threshold is None:
                similarity_threshold = self.default_similarity_threshold
            
            # 1. Encode question
            question_embedding = self.encode_text(question)
            
            # 2. FAISS search v·ªõi top_k cao h∆°n ƒë·ªÉ c√≥ nhi·ªÅu l·ª±a ch·ªçn
            search_k = min(top_k * 3, 50)  # T√¨m nhi·ªÅu h∆°n ƒë·ªÉ filter
            scores, indices = self.faiss_index.search(question_embedding.astype('float32'), search_k)
            
            # 3. T·∫°o k·∫øt qu·∫£ v√† filter
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.chunks_metadata):
                    chunk_meta = self.chunks_metadata[idx]
                    
                    # Filter theo category n·∫øu c√≥
                    if filter_category and filter_category != 'all':
                        if chunk_meta['category'] != filter_category:
                            continue
                    
                    # Filter theo similarity threshold
                    if score < similarity_threshold:
                        continue
                    
                    results.append({
                        'score': float(score),
                        'similarity': float(score),
                        'pdf_name': chunk_meta['pdf_name'],
                        'content': chunk_meta['content'],
                        'category': chunk_meta['category'],
                        'content_length': chunk_meta['content_length'],
                        'doc_idx': chunk_meta['doc_idx'],
                        'chunk_idx': chunk_meta['chunk_idx']
                    })
            
            # 4. S·∫Øp x·∫øp theo score v√† l·∫•y top_k
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói search chunks: {e}")
            return []
    
    def generate_comprehensive_answer(self, 
                                    question: str, 
                                    search_results: List[Dict],
                                    include_sources: bool = True) -> Dict[str, Any]:
        """T·∫°o c√¢u tr·∫£ l·ªùi to√†n di·ªán t·ª´ search results s·ª≠ d·ª•ng LLM"""
        try:
            if not search_results:
                return {
                    'answer': "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu.",
                    'confidence': 0.0,
                    'method': 'no_results'
                }
            
            # S·ª≠ d·ª•ng LLM n·∫øu c√≥ s·∫µn
            if self.use_llm_generation and self.llm_service:
                return self._generate_llm_answer(question, search_results)
            else:
                return self._generate_template_answer(question, search_results)
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói generate answer: {e}")
            return {
                'answer': "Xin l·ªói, c√≥ l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i.",
                'confidence': 0.0,
                'method': 'error'
            }
    
    def _clean_filename(self, filename: str) -> str:
        """L√†m s·∫°ch t√™n file ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp"""
        # Lo·∫°i b·ªè extension
        name = filename.replace('.pdf', '').replace('.PDF', '')
        
        # Thay th·∫ø underscore b·∫±ng space
        name = name.replace('_', ' ')
        
        # Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
        name = name.title()
        
        return name
    
    def _extract_important_sentences(self, content: str, question_lower: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√¢u quan tr·ªçng t·ª´ content d·ª±a tr√™n c√¢u h·ªèi"""
        
        # T√°ch content th√†nh sentences t·ªët h∆°n
        import re
        
        # T√°ch theo d·∫•u c√¢u v√† xu·ªëng d√≤ng
        sentences = re.split(r'[.!?]\s*|\n+', content)
        important_sentences = []
        
        # X√°c ƒë·ªãnh keywords d·ª±a tr√™n c√¢u h·ªèi
        topic_keywords = self._get_topic_keywords(question_lower)
        
        # Scoring v√† ranking sentences
        sentence_scores = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 20 or len(sentence) > 500:  # B·ªè qua c√¢u qu√° ng·∫Øn/d√†i
                continue
                
            sentence_lower = sentence.lower()
            score = 0
            
            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n keywords
            for keyword in topic_keywords:
                if keyword in sentence_lower:
                    score += len(keyword.split())  # Keyword d√†i h∆°n = ƒëi·ªÉm cao h∆°n
            
            # Bonus cho c√¢u c√≥ ƒë·ªãnh nghƒ©a
            if any(def_word in sentence_lower for def_word in ['l√†', 'ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a', 'c√≥ nghƒ©a', 'refers to']):
                score += 2
            
            # Penalty cho c√¢u c√≥ URL ho·∫∑c reference
            if any(ref in sentence_lower for ref in ['http', 'www', 'truy c·∫≠p', 'tham kh·∫£o']):
                score -= 1
            
            if score > 0:
                sentence_scores.append((sentence, score))
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm v√† l·∫•y top sentences
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # L·∫•y t·ªëi ƒëa 3 c√¢u t·ªët nh·∫•t
        for sentence, score in sentence_scores[:3]:
            important_sentences.append(sentence)
        
        return important_sentences
    
    def _get_topic_keywords(self, question_lower: str) -> List[str]:
        """L·∫•y keywords li√™n quan ƒë·∫øn topic trong c√¢u h·ªèi"""
        topic_keywords_map = {
            'ddos': ['ddos', 'dos', 'denial of service', 't·ª´ ch·ªëi d·ªãch v·ª•', 't·∫•n c√¥ng ph√¢n t√°n'],
            't∆∞·ªùng l·ª≠a': ['t∆∞·ªùng l·ª≠a', 'firewall', 'waf', 'ids', 'ips', 'ki·ªÉm so√°t truy c·∫≠p'],
            'malware': ['malware', 'virus', 'trojan', 'ransomware', 'm√£ ƒë·ªôc', 'ph·∫ßn m·ªÅm ƒë·ªôc h·∫°i'],
            'phishing': ['phishing', 'l·ª´a ƒë·∫£o', 'gi·∫£ m·∫°o', 'social engineering'],
            'm·∫≠t kh·∫©u': ['m·∫≠t kh·∫©u', 'password', 'authentication', 'x√°c th·ª±c'],
            'm√£ h√≥a': ['m√£ h√≥a', 'encryption', 'cryptography', 'm·∫≠t m√£'],
            'an to√†n th√¥ng tin': ['an to√†n th√¥ng tin', 'information security', 'b·∫£o m·∫≠t', 'cybersecurity']
        }
        
        # T√¨m keywords ph√π h·ª£p v·ªõi c√¢u h·ªèi
        relevant_keywords = []
        for topic, keywords in topic_keywords_map.items():
            if any(keyword in question_lower for keyword in keywords):
                relevant_keywords.extend(keywords)
        
        # Th√™m keywords chung
        relevant_keywords.extend(['ƒë·ªãnh nghƒ©a', 'kh√°i ni·ªám', 'b·∫£o v·ªá', 'security', 'r·ªßi ro', 'm·ªëi ƒëe d·ªça'])
        
        return list(set(relevant_keywords))  # Remove duplicates
    
    def _clean_sentence(self, sentence: str) -> str:
        """L√†m s·∫°ch c√¢u vƒÉn"""
        import re
        
        # Lo·∫°i b·ªè URLs
        sentence = re.sub(r'https?://[^\s]+', '', sentence)
        sentence = re.sub(r'www\.[^\s]+', '', sentence)
        
        # Lo·∫°i b·ªè references kh√¥ng ho√†n ch·ªânh
        sentence = re.sub(r'truy c·∫≠p v√†o th√°ng \d+.*', '', sentence)
        sentence = re.sub(r'tham kh·∫£o.*', '', sentence)
        
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát th·ª´a
        sentence = re.sub(r'[%#*]+', '', sentence)
        sentence = re.sub(r'\s+', ' ', sentence)  # Normalize whitespace
        
        # Lo·∫°i b·ªè d·∫•u c√¢u th·ª´a ·ªü cu·ªëi
        sentence = sentence.strip(' .,;:')
        
        return sentence
    
    def _create_content_summary(self, content: str, question_lower: str) -> str:
        """T·∫°o summary t·ª´ content khi kh√¥ng t√¨m ƒë∆∞·ª£c c√¢u quan tr·ªçng"""
        
        # T√¨m ƒëo·∫°n vƒÉn c√≥ ch·ª©a keywords li√™n quan
        topic_keywords = self._get_topic_keywords(question_lower)
        
        # T√°ch content th√†nh paragraphs
        paragraphs = content.split('\n')
        best_paragraph = ""
        best_score = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if len(paragraph) < 50:  # B·ªè qua ƒëo·∫°n qu√° ng·∫Øn
                continue
                
            paragraph_lower = paragraph.lower()
            score = 0
            
            # T√≠nh ƒëi·ªÉm cho paragraph
            for keyword in topic_keywords:
                score += paragraph_lower.count(keyword)
            
            if score > best_score:
                best_score = score
                best_paragraph = paragraph
        
        if best_paragraph:
            # Clean v√† truncate
            summary = self._clean_sentence(best_paragraph)
            if len(summary) > 200:
                summary = summary[:200] + "..."
            return summary
        
        # Fallback: l·∫•y ƒëo·∫°n ƒë·∫ßu
        first_part = content[:200].strip()
        return self._clean_sentence(first_part) + "..."
    
    async def query(self, 
                   question: str,
                   top_k: Optional[int] = None,
                   filter_category: Optional[str] = None,
                   include_sources: bool = True,
                   similarity_threshold: Optional[float] = None) -> Dict[str, Any]:
        """API ch√≠nh ƒë·ªÉ x·ª≠ l√Ω query v√† tr·∫£ v·ªÅ response"""
        try:
            start_time = time.time()
            
            if not self.is_initialized:
                await self.initialize()
            
            # Validate input
            if not question or not question.strip():
                raise ValueError("Question kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
            
            question = question.strip()
            logger.info(f"üîç Processing query: {question[:100]}...")
            
            # 1. Search relevant chunks
            search_results = self.search_relevant_chunks(
                question=question,
                top_k=top_k,
                filter_category=filter_category,
                similarity_threshold=similarity_threshold
            )
            
            # 2. Generate comprehensive answer
            answer_data = self.generate_comprehensive_answer(
                question=question,
                search_results=search_results,
                include_sources=include_sources
            )
            
            # 3. Prepare sources information
            sources = []
            if include_sources and search_results:
                for result in search_results:
                    sources.append({
                        'filename': result['pdf_name'],
                        'display_name': self._clean_filename(result['pdf_name']),
                        'category': result['category'],
                        'content_preview': result['content'][:300] + "..." if len(result['content']) > 300 else result['content'],
                        'similarity_score': result['similarity'],
                        'content_length': result['content_length']
                    })
            
            # 4. Calculate processing time
            processing_time = int((time.time() - start_time) * 1000)
            
            # 5. Prepare final response
            response = {
                'question': question,
                'answer': answer_data['answer'],
                'sources': sources,
                'total_sources': len(sources),
                'confidence': answer_data.get('confidence', 0.0),
                'method': answer_data.get('method', 'unified'),
                'processing_time_ms': processing_time,
                'filter_category': filter_category or 'all',
                'timestamp': datetime.now().isoformat(),
                'service_version': '1.0.0'
            }
            
            logger.info(f"‚úÖ Query processed successfully: {len(sources)} sources, {processing_time}ms")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong query unified: {e}")
            return {
                'question': question,
                'answer': f"Xin l·ªói, c√≥ l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}",
                'sources': [],
                'total_sources': 0,
                'confidence': 0.0,
                'method': 'error',
                'processing_time_ms': 0,
                'error': str(e)
            }
    
    async def get_service_stats(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ service"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # Th·ªëng k√™ theo category
            category_stats = {}
            for chunk in self.chunks_metadata:
                cat = chunk['category']
                if cat not in category_stats:
                    category_stats[cat] = {'chunks': 0, 'total_length': 0}
                category_stats[cat]['chunks'] += 1
                category_stats[cat]['total_length'] += chunk['content_length']
            
            return {
                'service_name': 'RAG Service Unified',
                'version': '1.0.0',
                'status': 'ready' if self.is_initialized else 'not_initialized',
                'initialization_time': self.initialization_time,
                'total_documents': self.total_documents,
                'total_chunks': self.total_chunks,
                'categories': category_stats,
                'device': str(self.device),
                'model_path': 'D:/Vian/MODELS/multilingual_e5_large',
                'default_settings': {
                    'top_k': self.default_top_k,
                    'similarity_threshold': self.default_similarity_threshold,
                    'max_answer_length': self.max_answer_length
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói get stats: {e}")
            return {'error': str(e)}

# Global instance
_rag_service_unified = None

async def get_rag_service_unified():
    """Get RAG Service Unified instance"""
    global _rag_service_unified
    if _rag_service_unified is None:
        _rag_service_unified = RAGServiceUnified()
        await _rag_service_unified.initialize()
    return _rag_service_unified
