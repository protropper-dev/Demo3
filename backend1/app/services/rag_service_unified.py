#!/usr/bin/env python3
"""
RAG Service Unified - API th·ªëng nh·∫•t cho query/response
H·ª£p nh·∫•t t·∫•t c·∫£ c√°c ch·ª©c nƒÉng RAG th√†nh m·ªôt service m·∫°nh m·∫Ω
"""

import os
import logging
import pickle
import faiss
import numpy as np
from typing import List, Dict, Any, Optional, Union
from transformers import AutoTokenizer, AutoModel
import torch
import time
from datetime import datetime
from app.services.llm_service import LLMService

logger = logging.getLogger(__name__)

class RAGServiceUnified:
    """RAG Service th·ªëng nh·∫•t - x·ª≠ l√Ω t·∫•t c·∫£ query/response"""
    
    def __init__(self):
        self.is_initialized = False
        self.tokenizer = None
        self.model = None
        self.device = None
        self.faiss_index = None
        self.documents_data = None
        self.chunks_metadata = []
        self.total_chunks = 0
        self.total_documents = 0
        self.initialization_time = None
        
        # LLM Service cho text generation
        self.llm_service = None
        
        # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
        self.default_top_k = 5
        self.default_similarity_threshold = 0.3
        self.max_answer_length = 2000
        self.max_context_length = 1500
        self.use_llm_generation = True  # T·∫°m th·ªùi disable ƒë·ªÉ test template - set True ƒë·ªÉ enable LLM
        
    async def initialize(self):
        """Kh·ªüi t·∫°o service v·ªõi d·ªØ li·ªáu c√≥ s·∫µn"""
        try:
            start_time = time.time()
            logger.info("üöÄ ƒêang kh·ªüi t·∫°o RAG Service Unified...")
            
            # 1. Load embedding model
            model_path = "models/multilingual_e5_large"
            logger.info(f"üì• Loading embedding model: {model_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModel.from_pretrained(model_path)
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model.to(self.device)
            
            logger.info(f"‚úÖ Embedding model loaded on {self.device}")
            
            # 2. Initialize LLM Service
            if self.use_llm_generation:
                try:
                    llm_model_path = "models/vinallama-2.7b-chat"
                    logger.info(f"üì• Initializing LLM service: {llm_model_path}")
                    
                    self.llm_service = LLMService(llm_model_path)
                    self.llm_service.load_model()
                    
                    logger.info("‚úÖ LLM service initialized successfully")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è LLM service failed to load: {e}")
                    logger.warning("Will use template-based responses instead")
                    self.use_llm_generation = False
            
            # 3. Load d·ªØ li·ªáu
            data_dir = "data"
            faiss_path = os.path.join(data_dir, "all_faiss.index")
            pickle_path = os.path.join(data_dir, "all_embeddings.pkl")
            
            logger.info(f"üì• Loading FAISS index: {faiss_path}")
            self.faiss_index = faiss.read_index(faiss_path)
            
            logger.info(f"üì• Loading documents data: {pickle_path}")
            with open(pickle_path, 'rb') as f:
                self.documents_data = pickle.load(f)
            
            # 3. T·∫°o chunks metadata v·ªõi category mapping
            logger.info("üîÑ Processing chunks metadata...")
            self.chunks_metadata = []
            
            for doc_idx, doc in enumerate(self.documents_data):
                pdf_name = doc.get('pdf_name', 'Unknown')
                chunks = doc.get('chunks', [])
                
                # X√°c ƒë·ªãnh category d·ª±a tr√™n pdf_name
                category = self._determine_category(pdf_name)
                
                for chunk_idx, chunk in enumerate(chunks):
                    self.chunks_metadata.append({
                        'doc_idx': doc_idx,
                        'chunk_idx': chunk_idx,
                        'pdf_name': pdf_name,
                        'content': chunk,
                        'category': category,
                        'content_length': len(chunk)
                    })
            
            self.total_chunks = len(self.chunks_metadata)
            self.total_documents = len(self.documents_data)
            
            end_time = time.time()
            self.initialization_time = end_time - start_time
            
            self.is_initialized = True
            
            logger.info(f"‚úÖ RAG Service Unified initialized successfully!")
            logger.info(f"üìä Stats: {self.total_documents} documents, {self.total_chunks} chunks")
            logger.info(f"‚è±Ô∏è Initialization time: {self.initialization_time:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o RAG Service Unified: {e}")
            raise
    
    def _extract_main_topic(self, question_lower: str) -> str:
        """Tr√≠ch xu·∫•t ch·ªß ƒë·ªÅ ch√≠nh t·ª´ c√¢u h·ªèi"""
        # Mapping keywords to topics
        topic_keywords = {
            't∆∞·ªùng l·ª≠a': ['t∆∞·ªùng l·ª≠a', 'firewall', 'waf', 'ids', 'ips'],
            'ddos': ['ddos', 'dos', 'denial of service', 't·ª´ ch·ªëi d·ªãch v·ª•'],
            'malware': ['malware', 'virus', 'trojan', 'ransomware', 'm√£ ƒë·ªôc'],
            'phishing': ['phishing', 'l·ª´a ƒë·∫£o', 'gi·∫£ m·∫°o'],
            'm·∫≠t kh·∫©u': ['m·∫≠t kh·∫©u', 'password', 'authentication'],
            'm√£ h√≥a': ['m√£ h√≥a', 'encryption', 'cryptography'],
            'iso 27001': ['iso 27001', 'iso27001'],
            'nist': ['nist', 'cybersecurity framework'],
            'an to√†n th√¥ng tin': ['an to√†n th√¥ng tin', 'information security', 'b·∫£o m·∫≠t']
        }
        
        # T√¨m topic ph√π h·ª£p nh·∫•t
        for topic, keywords in topic_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                return topic
        
        return 'general'
    
    def _generate_dynamic_title(self, main_topic: str, answer_style: str) -> str:
        """T·∫°o ti√™u ƒë·ªÅ ƒë·ªông d·ª±a tr√™n topic"""
        if answer_style == 'definition':
            if main_topic == 't∆∞·ªùng l·ª≠a':
                return "ƒê·ªãnh nghƒ©a T∆∞·ªùng l·ª≠a (Firewall)"
            elif main_topic == 'ddos':
                return "T·∫•n c√¥ng DDoS l√† g√¨"
            elif main_topic == 'malware':
                return "ƒê·ªãnh nghƒ©a Malware v√† M√£ ƒë·ªôc"
            elif main_topic == 'phishing':
                return "T·∫•n c√¥ng Phishing l√† g√¨"
            elif main_topic == 'm·∫≠t kh·∫©u':
                return "ƒê·ªãnh nghƒ©a v·ªÅ M·∫≠t kh·∫©u v√† X√°c th·ª±c"
            elif main_topic == 'm√£ h√≥a':
                return "M√£ h√≥a (Encryption) l√† g√¨"
            elif main_topic == 'iso 27001':
                return "Ti√™u chu·∫©n ISO 27001"
            elif main_topic == 'nist':
                return "NIST Cybersecurity Framework"
            else:
                return "ƒê·ªãnh nghƒ©a An to√†n th√¥ng tin"
        elif answer_style == 'regulation':
            return f"Quy ƒë·ªãnh ph√°p l√Ω v·ªÅ {main_topic.title()}"
        elif answer_style == 'standard':
            return f"Ti√™u chu·∫©n v·ªÅ {main_topic.title()}"
        elif answer_style == 'attack':
            return f"Th√¥ng tin v·ªÅ {main_topic.title()}"
        else:
            return f"Th√¥ng tin v·ªÅ {main_topic.title()}"
    
    def _generate_dynamic_conclusion(self, main_topic: str, answer_style: str) -> str:
        """T·∫°o k·∫øt lu·∫≠n ƒë·ªông d·ª±a tr√™n topic"""
        conclusions = {
            't∆∞·ªùng l·ª≠a': "T∆∞·ªùng l·ª≠a l√† h·ªá th·ªëng b·∫£o m·∫≠t m·∫°ng quan tr·ªçng, gi√∫p ki·ªÉm so√°t v√† l·ªçc l∆∞u l∆∞·ª£ng m·∫°ng, b·∫£o v·ªá h·ªá th·ªëng kh·ªèi c√°c truy c·∫≠p tr√°i ph√©p.",
            'ddos': "T·∫•n c√¥ng DDoS l√† m·ªôt trong nh·ªØng m·ªëi ƒëe d·ªça nghi√™m tr·ªçng nh·∫•t ƒë·ªëi v·ªõi h·ªá th·ªëng m·∫°ng, c√≥ th·ªÉ g√¢y gi√°n ƒëo·∫°n d·ªãch v·ª• v√† thi·ªát h·∫°i kinh t·∫ø l·ªõn.",
            'malware': "Malware l√† m·ªëi ƒëe d·ªça th∆∞·ªùng xuy√™n trong kh√¥ng gian m·∫°ng, c·∫ßn c√≥ bi·ªán ph√°p ph√≤ng ch·ªëng ƒëa l·ªõp ƒë·ªÉ b·∫£o v·ªá h·ªá th·ªëng.",
            'phishing': "Phishing l√† h√¨nh th·ª©c t·∫•n c√¥ng k·ªπ thu·∫≠t x√£ h·ªôi nguy hi·ªÉm, c·∫ßn n√¢ng cao nh·∫≠n th·ª©c ng∆∞·ªùi d√πng ƒë·ªÉ ph√≤ng ch·ªëng hi·ªáu qu·∫£.",
            'm·∫≠t kh·∫©u': "M·∫≠t kh·∫©u m·∫°nh v√† x√°c th·ª±c ƒëa y·∫øu t·ªë l√† n·ªÅn t·∫£ng c·ªßa b·∫£o m·∫≠t h·ªá th·ªëng th√¥ng tin.",
            'm√£ h√≥a': "M√£ h√≥a l√† c√¥ng ngh·ªá c·ªët l√µi ƒë·ªÉ b·∫£o v·ªá t√≠nh b·∫£o m·∫≠t v√† to√†n v·∫πn c·ªßa d·ªØ li·ªáu.",
            'iso 27001': "ISO 27001 l√† ti√™u chu·∫©n qu·ªëc t·∫ø quan tr·ªçng cho h·ªá th·ªëng qu·∫£n l√Ω an to√†n th√¥ng tin.",
            'nist': "NIST Framework cung c·∫•p h∆∞·ªõng d·∫´n to√†n di·ªán ƒë·ªÉ qu·∫£n l√Ω r·ªßi ro cybersecurity.",
            'an to√†n th√¥ng tin': "An to√†n th√¥ng tin l√† lƒ©nh v·ª±c b·∫£o v·ªá th√¥ng tin v√† h·ªá th·ªëng th√¥ng tin kh·ªèi c√°c m·ªëi ƒëe d·ªça, ƒë·∫£m b·∫£o t√≠nh b·∫£o m·∫≠t (Confidentiality), to√†n v·∫πn (Integrity) v√† s·∫µn s√†ng (Availability) c·ªßa d·ªØ li·ªáu."
        }
        
        return conclusions.get(main_topic, "ƒê√¢y l√† m·ªôt kh√≠a c·∫°nh quan tr·ªçng c·ªßa an to√†n th√¥ng tin c·∫ßn ƒë∆∞·ª£c hi·ªÉu r√µ v√† tri·ªÉn khai ƒë√∫ng c√°ch.")
    
    # ==================== PIPELINE 2-STAGE: RAG ‚Üí LLM ENHANCEMENT ====================
    
    def _generate_rag_response(self, question: str, search_results: List[Dict]) -> Dict[str, Any]:
        """Stage 1: T·∫°o response ban ƒë·∫ßu t·ª´ RAG"""
        try:
            logger.info(f"üìù Stage 1: Generating RAG response for: {question[:50]}...")
            
            # T·∫°o response t·ª´ template ho·∫∑c basic LLM
            if self.use_llm_generation and self.llm_service:
                # Basic LLM generation v·ªõi prompt ƒë∆°n gi·∫£n
                basic_response = self._generate_basic_llm_response(question, search_results)
            else:
                # Template-based response
                basic_response = self._generate_template_response(question, search_results)
            
            return {
                'raw_response': basic_response,
                'sources': search_results,
                'confidence': self._calculate_basic_confidence(search_results),
                'stage': 'rag_generation'
            }
            
        except Exception as e:
            logger.error(f"‚ùå RAG response generation error: {e}")
            # Fallback to template
            return {
                'raw_response': self._generate_template_response(question, search_results),
                'sources': search_results,
                'confidence': 0.5,
                'stage': 'rag_generation'
            }
    
    def _generate_basic_llm_response(self, question: str, search_results: List[Dict]) -> str:
        """T·∫°o response c∆° b·∫£n t·ª´ LLM v·ªõi prompt ƒë∆°n gi·∫£n"""
        try:
            # Simple prompt cho basic generation
            context_text = ""
            for i, result in enumerate(search_results[:3], 1):
                context_text += f"\n=== Ngu·ªìn {i}: {result['pdf_name']} ===\n{result['content'][:400]}\n"
            
            basic_prompt = f"""
            D·ª±a tr√™n c√°c t√†i li·ªáu sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi: {question}
            
            T√†i li·ªáu:
            {context_text}
            
            Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c:
            """
            
            # C·∫•u h√¨nh generation cho basic response
            basic_generation_config = {
                'temperature': 0.6,
                'max_new_tokens': 200,
                'top_p': 0.9,
                'top_k': 50,
                'repetition_penalty': 1.1
            }
            
            response = self.llm_service.generate_response(
                query=question,
                context_docs=search_results[:3],
                max_new_tokens=200,
                generation_config=basic_generation_config
            )
            return response
            
        except Exception as e:
            logger.warning(f"Basic LLM generation failed: {e}")
            return self._generate_template_response(question, search_results)
    
    def _generate_template_response(self, question: str, search_results: List[Dict]) -> str:
        """T·∫°o response t·ª´ template (fallback)"""
        if not search_results:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n."
        
        # L·∫•y top result
        top_result = search_results[0]
        content = top_result['content'][:500]  # Gi·ªõi h·∫°n ƒë·ªô d√†i
        
        # T·∫°o response c∆° b·∫£n
        response = f"D·ª±a tr√™n t√†i li·ªáu {top_result['pdf_name']}, {content}"
        
        return response
    
    def _calculate_basic_confidence(self, search_results: List[Dict]) -> float:
        """T√≠nh confidence c∆° b·∫£n cho RAG response"""
        if not search_results:
            return 0.0
        
        # Average similarity score
        avg_similarity = sum(r['similarity'] for r in search_results[:3]) / min(len(search_results), 3)
        
        # Diversity bonus
        categories = set(r['category'] for r in search_results[:3])
        diversity_bonus = min(len(categories) * 0.1, 0.2)
        
        return min(avg_similarity + diversity_bonus, 1.0)
    
    def _enhance_response_with_llm(self, rag_response: Dict[str, Any], question: str) -> Dict[str, Any]:
        """Stage 2: N√¢ng cao ch·∫•t l∆∞·ª£ng response b·∫±ng LLM"""
        try:
            logger.info(f"üöÄ Stage 2: Enhancing response with LLM...")
            
            # 1. Chu·∫©n b·ªã context cho enhancement
            enhancement_context = self._prepare_enhancement_context(rag_response, question)
            
            # 2. T·∫°o enhancement prompt
            enhancement_prompt = self._create_enhancement_prompt(rag_response, question)
            
            # 3. G·ªçi LLM ƒë·ªÉ enhance response
            enhanced_response = self._call_llm_for_enhancement(enhancement_prompt)
            
            # 4. Validate enhanced response
            validated_response = self._validate_enhanced_response(enhanced_response, rag_response)
            
            # 5. T√≠nh confidence m·ªõi
            enhanced_confidence = self._calculate_enhanced_confidence(rag_response, validated_response)
            
            return {
                'original_response': rag_response['raw_response'],
                'enhanced_response': validated_response,
                'sources': rag_response['sources'],
                'confidence': enhanced_confidence,
                'enhancement_applied': True,
                'stage': 'llm_enhancement'
            }
            
        except Exception as e:
            logger.warning(f"LLM enhancement failed: {e}")
            # Fallback to original response
            return {
                'original_response': rag_response['raw_response'],
                'enhanced_response': rag_response['raw_response'],
                'sources': rag_response['sources'],
                'confidence': rag_response['confidence'],
                'enhancement_applied': False,
                'stage': 'rag_generation'
            }
    
    def _prepare_enhancement_context(self, rag_response: Dict[str, Any], question: str) -> Dict[str, Any]:
        """Chu·∫©n b·ªã context cho enhancement"""
        return {
            'original_response': rag_response['raw_response'],
            'question': question,
            'sources': rag_response['sources'],
            'original_confidence': rag_response['confidence'],
            'response_length': len(rag_response['raw_response']),
            'sources_count': len(rag_response['sources']),
            'question_type': self._classify_question_type(question)
        }
    
    def _classify_question_type(self, query: str) -> str:
        """Ph√¢n lo·∫°i c√¢u h·ªèi ƒë·ªÉ t·ªëi ∆∞u prompt"""
        query_lower = query.lower()
        
        # Keywords cho t·ª´ng lo·∫°i
        definition_keywords = ['l√† g√¨', 'what is', 'ƒë·ªãnh nghƒ©a', 'kh√°i ni·ªám', 'nghƒ©a l√†']
        regulation_keywords = ['lu·∫≠t', 'quy ƒë·ªãnh', 'regulation', 'law', 'ƒëi·ªÅu', 'kho·∫£n']
        standard_keywords = ['iso', 'nist', 'ti√™u chu·∫©n', 'standard', 'chu·∫©n']
        attack_keywords = ['t·∫•n c√¥ng', 'attack', 'hack', 'malware', 'virus', 'exploit']
        howto_keywords = ['l√†m th·∫ø n√†o', 'how to', 'c√°ch', 'h∆∞·ªõng d·∫´n', 'th·ª±c hi·ªán']
        
        if any(keyword in query_lower for keyword in definition_keywords):
            return 'definition'
        elif any(keyword in query_lower for keyword in regulation_keywords):
            return 'regulation'
        elif any(keyword in query_lower for keyword in standard_keywords):
            return 'standard'
        elif any(keyword in query_lower for keyword in attack_keywords):
            return 'attack'
        elif any(keyword in query_lower for keyword in howto_keywords):
            return 'howto'
        else:
            return 'general'
    
    def _create_enhancement_prompt(self, rag_response: Dict[str, Any], question: str) -> str:
        """T·∫°o prompt cho LLM enhancement - t·ªëi ∆∞u cho ti·∫øng Vi·ªát"""
        original_response = rag_response['raw_response']
        sources = rag_response['sources']
        question_type = self._classify_question_type(question)
        
        # System prompt cho enhancement - t·ªëi ∆∞u cho ti·∫øng Vi·ªát
        system_prompt = f"""B·∫°n l√† chuy√™n gia an to√†n th√¥ng tin v√† c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:

**Y√äU C·∫¶U CH√çNH:**
1. **S·ª≠a l·ªói ch√≠nh t·∫£ v√† ng·ªØ ph√°p** - ƒê·∫£m b·∫£o ti·∫øng Vi·ªát chu·∫©n, kh√¥ng c√≥ l·ªói ch√≠nh t·∫£
2. **C·∫£i thi·ªán c·∫•u tr√∫c** - S·ª≠ d·ª•ng d·∫•u c√¢u ƒë√∫ng, xu·ªëng d√≤ng h·ª£p l√Ω, c√¢u ho√†n ch·ªânh
3. **L√†m r√µ n·ªôi dung** - Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu, logic
4. **B·ªï sung th√¥ng tin** - Th√™m th√¥ng tin quan tr·ªçng t·ª´ sources n·∫øu c·∫ßn
5. **T·ªëi ∆∞u ƒë·ªô d√†i** - T·ªëi ƒëa 300 t·ª´, s√∫c t√≠ch nh∆∞ng ƒë·∫ßy ƒë·ªß

**QUY T·∫ÆC VI·∫æT:**
- S·ª≠ d·ª•ng thu·∫≠t ng·ªØ k·ªπ thu·∫≠t ch√≠nh x√°c (DDoS, t·∫•n c√¥ng, l∆∞u l∆∞·ª£ng, m√°y ch·ªß, ngƒÉn ch·∫∑n)
- Vi·∫øt c√¢u ho√†n ch·ªânh, c√≥ ch·ªß ng·ªØ v√† v·ªã ng·ªØ
- S·ª≠ d·ª•ng d·∫•u c√¢u ƒë√∫ng: d·∫•u ch·∫•m, ph·∫©y, hai ch·∫•m
- Tr√°nh l·∫∑p t·ª´, l·∫∑p c·ª•m t·ª´
- C·∫•u tr√∫c r√µ r√†ng: ƒë·ªãnh nghƒ©a ‚Üí ƒë·∫∑c ƒëi·ªÉm ‚Üí v√≠ d·ª• ‚Üí gi·∫£i ph√°p

**LO·∫†I C√ÇU H·ªéI:** {question_type}"""

        # Context t·ª´ sources - r√∫t g·ªçn ƒë·ªÉ tr√°nh qu√° d√†i
        sources_context = ""
        for i, source in enumerate(sources[:2], 1):  # Ch·ªâ l·∫•y 2 sources ƒë·∫ßu
            sources_context += f"\n**Ngu·ªìn {i}:** {source['pdf_name']}\n{source['content'][:300]}...\n"
        
        # User prompt - ƒë∆°n gi·∫£n v√† r√µ r√†ng
        user_prompt = f"""**C√¢u h·ªèi:** {question}

**C√¢u tr·∫£ l·ªùi hi·ªán t·∫°i c·∫ßn c·∫£i thi·ªán:**
{original_response}

**Ngu·ªìn t√†i li·ªáu tham kh·∫£o:**
{sources_context}

**H√£y c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi v·ªõi y√™u c·∫ßu:**
- S·ª≠a t·∫•t c·∫£ l·ªói ch√≠nh t·∫£ v√† ng·ªØ ph√°p
- Vi·∫øt l·∫°i v·ªõi c·∫•u tr√∫c r√µ r√†ng, d·ªÖ hi·ªÉu
- S·ª≠ d·ª•ng ti·∫øng Vi·ªát chu·∫©n, t·ª± nhi√™n
- T·ªëi ƒëa 300 t·ª´, s√∫c t√≠ch nh∆∞ng ƒë·∫ßy ƒë·ªß
- Gi·ªØ nguy√™n th√¥ng tin k·ªπ thu·∫≠t ch√≠nh x√°c

**C√¢u tr·∫£ l·ªùi c·∫£i thi·ªán:**"""
        
        return f"<|im_start|>system\n{system_prompt}\n<|im_end|>\n<|im_start|>user\n{user_prompt}\n<|im_end|>\n<|im_start|>assistant\n"
    
    def _call_llm_for_enhancement(self, enhancement_prompt: str) -> str:
        """G·ªçi LLM ƒë·ªÉ enhance response"""
        try:
            # C·∫•u h√¨nh t·ªëi ∆∞u cho enhancement - t·∫≠p trung v√†o ch·∫•t l∆∞·ª£ng
            generation_config = {
                'temperature': 0.4,  # Gi·∫£m ƒë·ªÉ ·ªïn ƒë·ªãnh h∆°n, gi·∫£m l·ªói ch√≠nh t·∫£
                'top_p': 0.75,  # Gi·∫£m ƒë·ªÉ t·∫≠p trung h∆°n
                'top_k': 30,  # Gi·∫£m ƒë·ªÉ ·ªïn ƒë·ªãnh h∆°n
                'repetition_penalty': 1.25,  # TƒÉng ƒë·ªÉ tr√°nh l·∫∑p t·ª´
                'no_repeat_ngram_size': 4,  # TƒÉng ƒë·ªÉ tr√°nh l·∫∑p c·ª•m t·ª´
                'max_new_tokens': 350,  # Gi·∫£m ƒë·ªÉ tr√°nh qu√° d√†i
                'do_sample': True,
                'early_stopping': True
            }
            
            # G·ªçi LLM v·ªõi prompt enhancement
            enhanced_response = self.llm_service.generate_response(
                query=enhancement_prompt,
                context_docs=None,  # Kh√¥ng c·∫ßn context docs v√¨ ƒë√£ c√≥ trong prompt
                max_new_tokens=generation_config['max_new_tokens'],
                generation_config=generation_config
            )
            
            return enhanced_response
            
        except Exception as e:
            logger.error(f"LLM enhancement call failed: {e}")
            raise
    
    def _validate_enhanced_response(self, enhanced_response: str, rag_response: Dict[str, Any]) -> str:
        """Validate enhanced response"""
        original_response = rag_response['raw_response']
        
        # 1. Length validation - t·ªëi ∆∞u cho ti·∫øng Vi·ªát
        if len(enhanced_response.strip()) < 30:
            logger.warning("Enhanced response too short, using original")
            return original_response
        
        if len(enhanced_response.strip()) > 600:
            logger.warning("Enhanced response too long, truncating")
            enhanced_response = enhanced_response[:600] + "..."
        
        # 2. Content validation
        if not self._contains_key_information(enhanced_response, original_response):
            logger.warning("Enhanced response missing key information, using original")
            return original_response
        
        # 3. Quality validation
        if self._has_quality_issues(enhanced_response):
            logger.warning("Enhanced response has quality issues, using original")
            return original_response
        
        return enhanced_response
    
    def _contains_key_information(self, enhanced: str, original: str) -> bool:
        """Ki·ªÉm tra enhanced response c√≥ ch·ª©a th√¥ng tin ch√≠nh t·ª´ original"""
        # Extract key phrases from original
        original_keywords = self._extract_key_phrases(original)
        
        # Check if enhanced contains most key phrases
        enhanced_lower = enhanced.lower()
        matches = sum(1 for keyword in original_keywords if keyword.lower() in enhanced_lower)
        
        return matches >= len(original_keywords) * 0.7  # 70% match required
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """Extract key phrases from text"""
        # Simple key phrase extraction
        words = text.split()
        # Filter out common words and get meaningful phrases
        key_phrases = []
        for word in words:
            if len(word) > 3 and word.lower() not in ['c·ªßa', 'v·ªõi', 't·ª´', 'cho', 'ƒë∆∞·ª£c', 'c√≥', 'l√†', 'v√†', 'trong', 'theo']:
                key_phrases.append(word)
        return key_phrases[:10]  # Top 10 key phrases
    
    def _has_quality_issues(self, response: str) -> bool:
        """Ki·ªÉm tra quality issues"""
        # Check for error indicators
        error_indicators = ['kh√¥ng th·ªÉ', 'th·ª≠ l·∫°i', 'l·ªói', 'error', 'sorry', 'cannot']
        if any(indicator in response.lower() for indicator in error_indicators):
            return True
        
        # Check for repetition
        words = response.split()
        if len(words) > 10:
            word_counts = {}
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
            
            # Check for excessive repetition
            max_repetition = max(word_counts.values())
            if max_repetition > len(words) * 0.3:  # 30% repetition threshold
                return True
        
        return False
    
    def _calculate_enhanced_confidence(self, rag_response: Dict[str, Any], enhanced_response: str) -> float:
        """T√≠nh confidence cho enhanced response"""
        original_confidence = rag_response['confidence']
        
        # 1. Base confidence t·ª´ original
        base_confidence = original_confidence
        
        # 2. Enhancement quality bonus
        enhancement_bonus = self._calculate_enhancement_bonus(rag_response['raw_response'], enhanced_response)
        
        # 3. Response quality bonus
        quality_bonus = self._calculate_quality_bonus(enhanced_response)
        
        # 4. Source utilization bonus
        source_bonus = self._calculate_source_utilization_bonus(enhanced_response, rag_response['sources'])
        
        # Calculate final confidence
        final_confidence = base_confidence + enhancement_bonus + quality_bonus + source_bonus
        
        return min(max(final_confidence, 0.0), 1.0)
    
    def _calculate_enhancement_bonus(self, original: str, enhanced: str) -> float:
        """T√≠nh bonus cho enhancement quality"""
        # Length improvement
        length_ratio = len(enhanced) / max(len(original), 1)
        if 1.2 <= length_ratio <= 2.0:  # Optimal length increase
            length_bonus = 0.1
        elif length_ratio > 2.0:  # Too long
            length_bonus = -0.05
        else:  # Too short
            length_bonus = -0.05
        
        # Structure improvement
        structure_bonus = 0.0
        if any(marker in enhanced for marker in ['**', '###', '-', '1.', '2.']):
            structure_bonus = 0.05
        
        # Language improvement
        language_bonus = 0.0
        if self._has_better_language(enhanced, original):
            language_bonus = 0.05
        
        return length_bonus + structure_bonus + language_bonus
    
    def _has_better_language(self, enhanced: str, original: str) -> bool:
        """Ki·ªÉm tra enhanced c√≥ ng√¥n ng·ªØ t·ªët h∆°n kh√¥ng"""
        # Simple check for better language indicators
        better_indicators = ['d·ª±a tr√™n', 'theo', 'c·ª• th·ªÉ', 'chi ti·∫øt', 'v√≠ d·ª•']
        return any(indicator in enhanced.lower() for indicator in better_indicators)
    
    def _calculate_quality_bonus(self, response: str) -> float:
        """T√≠nh bonus cho response quality"""
        quality_bonus = 0.0
        
        # Completeness
        if len(response) >= 200:
            quality_bonus += 0.05
        
        # Structure
        if any(marker in response for marker in ['**', '###', '-', '1.', '2.']):
            quality_bonus += 0.05
        
        # Professional language
        if self._has_professional_language(response):
            quality_bonus += 0.05
        
        return quality_bonus
    
    def _has_professional_language(self, response: str) -> bool:
        """Ki·ªÉm tra ng√¥n ng·ªØ chuy√™n nghi·ªáp"""
        professional_indicators = ['theo quy ƒë·ªãnh', 'cƒÉn c·ª©', 'd·ª±a tr√™n', 'theo ti√™u chu·∫©n']
        return any(indicator in response.lower() for indicator in professional_indicators)
    
    def _calculate_source_utilization_bonus(self, response: str, sources: List[Dict]) -> float:
        """T√≠nh bonus cho vi·ªác s·ª≠ d·ª•ng sources"""
        if not sources:
            return 0.0
        
        # Check if response mentions source information
        source_mentions = 0
        for source in sources[:3]:
            filename = source['pdf_name']
            if any(word in response.lower() for word in filename.lower().split()):
                source_mentions += 1
        
        return min(source_mentions * 0.05, 0.15)  # Max 0.15 bonus

    def _generate_llm_answer(self, question: str, search_results: List[Dict]) -> Dict[str, Any]:
        """T·∫°o c√¢u tr·∫£ l·ªùi s·ª≠ d·ª•ng LLM v·ªõi RAG context (Legacy method - kept for compatibility)"""
        try:
            logger.info(f"ü§ñ Attempting LLM generation for: {question[:50]}...")
            
            # Ki·ªÉm tra LLM service
            if not self.llm_service:
                logger.warning("‚ùå LLM service not available, using template")
                return self._generate_template_answer(question, search_results)
            
            # Chu·∫©n b·ªã context t·ª´ search results
            context_docs = []
            for result in search_results[:3]:  # Top 3 results
                context_docs.append({
                    'content': result['content'][:800],  # Gi·ªõi h·∫°n length ƒë·ªÉ tr√°nh token limit
                    'metadata': {
                        'filename': result['pdf_name'],
                        'category': result['category'],
                        'similarity': result['similarity']
                    }
                })
            
            logger.info(f"üìñ Prepared {len(context_docs)} context docs for LLM")
            
            # S·ª≠ d·ª•ng LLM service ƒë·ªÉ generate response
            llm_response = self.llm_service.generate_response(
                query=question,
                context_docs=context_docs,
                max_new_tokens=256  # Gi·∫£m ƒë·ªÉ tr√°nh timeout
            )
            
            # Ki·ªÉm tra response quality
            if not llm_response or len(llm_response.strip()) < 20:
                logger.warning("‚ùå LLM response too short, using template fallback")
                return self._generate_template_answer(question, search_results)
            
            if "kh√¥ng th·ªÉ" in llm_response or "th·ª≠ l·∫°i" in llm_response:
                logger.warning("‚ùå LLM returned error message, using template fallback")
                return self._generate_template_answer(question, search_results)
            
            # T√≠nh confidence d·ª±a tr√™n search results quality
            confidence = self._calculate_confidence(search_results, max_sources=3)
            
            logger.info(f"‚úÖ LLM generation successful: {len(llm_response)} chars")
            
            return {
                'answer': llm_response,
                'confidence': confidence,
                'method': 'llm_generation',
                'sources_used': len(search_results[:3])
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói LLM generation: {e}")
            logger.info("üîÑ Falling back to template generation")
            # Fallback to template
            return self._generate_template_answer(question, search_results)
    
    def _generate_template_answer(self, question: str, search_results: List[Dict]) -> Dict[str, Any]:
        """T·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng template (fallback)"""
        try:
            # 1. Ph√¢n t√≠ch c√¢u h·ªèi ƒë·ªÉ x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ v√† style
            question_lower = question.lower()
            
            # X√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh t·ª´ c√¢u h·ªèi
            main_topic = self._extract_main_topic(question_lower)
            
            # X√°c ƒë·ªãnh style d·ª±a tr√™n c√¢u h·ªèi
            is_definition = any(word in question_lower for word in ['l√† g√¨', 'what is', 'ƒë·ªãnh nghƒ©a', 'kh√°i ni·ªám'])
            is_regulation = any(word in question_lower for word in ['lu·∫≠t', 'quy ƒë·ªãnh', 'regulation', 'law'])
            is_standard = any(word in question_lower for word in ['iso', 'nist', 'ti√™u chu·∫©n', 'standard'])
            is_attack = any(word in question_lower for word in ['t·∫•n c√¥ng', 'attack', 'hack', 'malware', 'virus'])
            
            # 2. L·∫•y top results d·ª±a tr√™n lo·∫°i c√¢u h·ªèi
            if is_definition:
                top_results = search_results[:3]
                answer_style = 'definition'
            elif is_regulation:
                top_results = search_results[:4]
                answer_style = 'regulation'
            elif is_standard:
                top_results = search_results[:3]
                answer_style = 'standard'
            elif is_attack:
                top_results = search_results[:3]
                answer_style = 'attack'
            else:
                top_results = search_results[:3]
                answer_style = 'general'
            
            # 3. T·∫°o ti√™u ƒë·ªÅ ƒë·ªông d·ª±a tr√™n topic v√† style
            answer_parts = []
            title = self._generate_dynamic_title(main_topic, answer_style)
            answer_parts.append(f"**{title}:**")
            answer_parts.append("")
            
            # 4. Th√™m n·ªôi dung t·ª´ t·ª´ng source
            for i, result in enumerate(top_results, 1):
                pdf_name = result['pdf_name']
                content = result['content']
                category = result['category']
                
                # L√†m s·∫°ch t√™n file ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n
                display_name = self._clean_filename(pdf_name)
                
                # T√¨m c√¢u quan tr·ªçng nh·∫•t
                important_sentences = self._extract_important_sentences(content, question_lower)
                
                if important_sentences:
                    answer_parts.append(f"**{i}. Theo {display_name}:**")
                    for sentence in important_sentences[:2]:  # T·ªëi ƒëa 2 c√¢u
                        cleaned_sentence = self._clean_sentence(sentence.strip())
                        if cleaned_sentence and len(cleaned_sentence) > 20:
                            answer_parts.append(f"‚Ä¢ {cleaned_sentence}")
                    answer_parts.append("")
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y c√¢u c·ª• th·ªÉ, t·∫°o summary t·ª´ content
                    summary = self._create_content_summary(content, question_lower)
                    if summary:
                        answer_parts.append(f"**{i}. Theo {display_name}:**")
                        answer_parts.append(f"‚Ä¢ {summary}")
                        answer_parts.append("")
            
            # 5. Th√™m t·ªïng k·∫øt ƒë·ªông d·ª±a tr√™n topic
            answer_parts.append("---")
            conclusion = self._generate_dynamic_conclusion(main_topic, answer_style)
            answer_parts.append(f"**T√≥m l·∫°i:** {conclusion}")
            
            answer_text = "\n".join(answer_parts)
            
            # 6. T√≠nh confidence d·ª±a tr√™n quality c·ªßa results
            confidence = self._calculate_confidence(top_results, max_sources=len(top_results))
            
            return {
                'answer': answer_text,
                'confidence': confidence,
                'method': answer_style + '_template',
                'sources_used': len(top_results)
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói template generation: {e}")
            return {
                'answer': "Xin l·ªói, c√≥ l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i.",
                'confidence': 0.0,
                'method': 'error'
            }
    
    def _calculate_confidence(self, search_results: List[Dict], max_sources: int = 3) -> float:
        """
        T√≠nh confidence d·ª±a tr√™n ch·∫•t l∆∞·ª£ng search results
        
        Logic ƒë√∫ng: Score th·∫•p (t·ªët) ‚Üí Confidence cao
        Score cao (k√©m) ‚Üí Confidence th·∫•p
        
        Args:
            search_results: Danh s√°ch k·∫øt qu·∫£ t√¨m ki·∫øm
            max_sources: S·ªë l∆∞·ª£ng sources t·ªëi ƒëa ƒë·ªÉ t√≠nh confidence
            
        Returns:
            float: Confidence score t·ª´ 0.0 ƒë·∫øn 1.0
        """
        try:
            if not search_results:
                return 0.0
            
            # L·∫•y top results ƒë·ªÉ t√≠nh confidence
            top_results = search_results[:max_sources]
            if not top_results:
                return 0.0
            
            # T√≠nh ƒëi·ªÉm trung b√¨nh
            avg_score = sum(r['score'] for r in top_results) / len(top_results)
            
            # Chuy·ªÉn ƒë·ªïi: score th·∫•p = confidence cao
            # Gi·∫£ s·ª≠ score t·ªët nh·∫•t l√† 0, t·ªá nh·∫•t l√† 200
            # C√¥ng th·ª©c: confidence = max(0, 1 - (avg_score / 200))
            confidence = max(0.0, 1.0 - (avg_score / 200.0))
            
            # ƒê·∫£m b·∫£o confidence kh√¥ng v∆∞·ª£t qu√° 1.0
            confidence = min(confidence, 1.0)
            
            logger.info(f"üìä Confidence calculation: avg_score={avg_score:.2f}, confidence={confidence:.3f}")
            
            return confidence
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói t√≠nh confidence: {e}")
            return 0.0
    
    def _determine_category(self, pdf_name: str) -> str:
        """X√°c ƒë·ªãnh category d·ª±a tr√™n t√™n file"""
        pdf_lower = pdf_name.lower()
        
        if any(keyword in pdf_lower for keyword in ['luat', 'nghi_dinh', 'quyet_dinh', 'thong_tu', 'tcvn']):
            return 'luat'
        elif any(keyword in pdf_lower for keyword in ['nist', 'iso', 'cybersecurity', 'framework']):
            return 'english'
        else:
            return 'vietnamese'
    
    def encode_text(self, text: str) -> np.ndarray:
        """Encode text th√†nh embedding vector"""
        try:
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                return embeddings.cpu().numpy()
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói encode text: {e}")
            raise
    
    def search_relevant_chunks(self, 
                             question: str, 
                             top_k: int = None,
                             filter_category: Optional[str] = None,
                             similarity_threshold: float = None) -> List[Dict]:
        """T√¨m ki·∫øm chunks li√™n quan v·ªõi filtering n√¢ng cao"""
        try:
            if not self.is_initialized:
                raise RuntimeError("Service ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            
            if top_k is None:
                top_k = self.default_top_k
            if similarity_threshold is None:
                similarity_threshold = self.default_similarity_threshold
            
            # 1. Encode question
            question_embedding = self.encode_text(question)
            
            # 2. FAISS search v·ªõi top_k cao h∆°n ƒë·ªÉ c√≥ nhi·ªÅu l·ª±a ch·ªçn
            search_k = min(top_k * 3, 50)  # T√¨m nhi·ªÅu h∆°n ƒë·ªÉ filter
            scores, indices = self.faiss_index.search(question_embedding.astype('float32'), search_k)
            
            # 3. T·∫°o k·∫øt qu·∫£ v√† filter
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.chunks_metadata):
                    chunk_meta = self.chunks_metadata[idx]
                    
                    # Filter theo category n·∫øu c√≥
                    if filter_category and filter_category != 'all':
                        if chunk_meta['category'] != filter_category:
                            continue
                    
                    # Filter theo similarity threshold
                    if score < similarity_threshold:
                        continue
                    
                    results.append({
                        'score': float(score),
                        'similarity': float(score),
                        'pdf_name': chunk_meta['pdf_name'],
                        'content': chunk_meta['content'],
                        'category': chunk_meta['category'],
                        'content_length': chunk_meta['content_length'],
                        'doc_idx': chunk_meta['doc_idx'],
                        'chunk_idx': chunk_meta['chunk_idx']
                    })
            
            # 4. S·∫Øp x·∫øp theo score v√† l·∫•y top_k
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói search chunks: {e}")
            return []
    
    def generate_comprehensive_answer(self, 
                                    question: str, 
                                    search_results: List[Dict],
                                    include_sources: bool = True) -> Dict[str, Any]:
        """T·∫°o c√¢u tr·∫£ l·ªùi to√†n di·ªán t·ª´ search results s·ª≠ d·ª•ng LLM"""
        try:
            if not search_results:
                return {
                    'answer': "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu.",
                    'confidence': 0.0,
                    'method': 'no_results'
                }
            
            # S·ª≠ d·ª•ng LLM n·∫øu c√≥ s·∫µn
            if self.use_llm_generation and self.llm_service:
                return self._generate_llm_answer(question, search_results)
            else:
                return self._generate_template_answer(question, search_results)
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói generate answer: {e}")
            return {
                'answer': "Xin l·ªói, c√≥ l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i.",
                'confidence': 0.0,
                'method': 'error'
            }
    
    def _clean_filename(self, filename: str) -> str:
        """L√†m s·∫°ch t√™n file ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp"""
        # Lo·∫°i b·ªè extension
        name = filename.replace('.pdf', '').replace('.PDF', '')
        
        # Thay th·∫ø underscore b·∫±ng space
        name = name.replace('_', ' ')
        
        # Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
        name = name.title()
        
        return name
    
    def _extract_important_sentences(self, content: str, question_lower: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√¢u quan tr·ªçng t·ª´ content d·ª±a tr√™n c√¢u h·ªèi"""
        
        # T√°ch content th√†nh sentences t·ªët h∆°n
        import re
        
        # T√°ch theo d·∫•u c√¢u v√† xu·ªëng d√≤ng
        sentences = re.split(r'[.!?]\s*|\n+', content)
        important_sentences = []
        
        # X√°c ƒë·ªãnh keywords d·ª±a tr√™n c√¢u h·ªèi
        topic_keywords = self._get_topic_keywords(question_lower)
        
        # Scoring v√† ranking sentences
        sentence_scores = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 20 or len(sentence) > 500:  # B·ªè qua c√¢u qu√° ng·∫Øn/d√†i
                continue
                
            sentence_lower = sentence.lower()
            score = 0
            
            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n keywords
            for keyword in topic_keywords:
                if keyword in sentence_lower:
                    score += len(keyword.split())  # Keyword d√†i h∆°n = ƒëi·ªÉm cao h∆°n
            
            # Bonus cho c√¢u c√≥ ƒë·ªãnh nghƒ©a
            if any(def_word in sentence_lower for def_word in ['l√†', 'ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a', 'c√≥ nghƒ©a', 'refers to']):
                score += 2
            
            # Penalty cho c√¢u c√≥ URL ho·∫∑c reference
            if any(ref in sentence_lower for ref in ['http', 'www', 'truy c·∫≠p', 'tham kh·∫£o']):
                score -= 1
            
            if score > 0:
                sentence_scores.append((sentence, score))
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm v√† l·∫•y top sentences
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # L·∫•y t·ªëi ƒëa 3 c√¢u t·ªët nh·∫•t
        for sentence, score in sentence_scores[:3]:
            important_sentences.append(sentence)
        
        return important_sentences
    
    def _get_topic_keywords(self, question_lower: str) -> List[str]:
        """L·∫•y keywords li√™n quan ƒë·∫øn topic trong c√¢u h·ªèi"""
        topic_keywords_map = {
            'ddos': ['ddos', 'dos', 'denial of service', 't·ª´ ch·ªëi d·ªãch v·ª•', 't·∫•n c√¥ng ph√¢n t√°n'],
            't∆∞·ªùng l·ª≠a': ['t∆∞·ªùng l·ª≠a', 'firewall', 'waf', 'ids', 'ips', 'ki·ªÉm so√°t truy c·∫≠p'],
            'malware': ['malware', 'virus', 'trojan', 'ransomware', 'm√£ ƒë·ªôc', 'ph·∫ßn m·ªÅm ƒë·ªôc h·∫°i'],
            'phishing': ['phishing', 'l·ª´a ƒë·∫£o', 'gi·∫£ m·∫°o', 'social engineering'],
            'm·∫≠t kh·∫©u': ['m·∫≠t kh·∫©u', 'password', 'authentication', 'x√°c th·ª±c'],
            'm√£ h√≥a': ['m√£ h√≥a', 'encryption', 'cryptography', 'm·∫≠t m√£'],
            'an to√†n th√¥ng tin': ['an to√†n th√¥ng tin', 'information security', 'b·∫£o m·∫≠t', 'cybersecurity']
        }
        
        # T√¨m keywords ph√π h·ª£p v·ªõi c√¢u h·ªèi
        relevant_keywords = []
        for topic, keywords in topic_keywords_map.items():
            if any(keyword in question_lower for keyword in keywords):
                relevant_keywords.extend(keywords)
        
        # Th√™m keywords chung
        relevant_keywords.extend(['ƒë·ªãnh nghƒ©a', 'kh√°i ni·ªám', 'b·∫£o v·ªá', 'security', 'r·ªßi ro', 'm·ªëi ƒëe d·ªça'])
        
        return list(set(relevant_keywords))  # Remove duplicates
    
    def _clean_sentence(self, sentence: str) -> str:
        """L√†m s·∫°ch c√¢u vƒÉn"""
        import re
        
        # Lo·∫°i b·ªè URLs
        sentence = re.sub(r'https?://[^\s]+', '', sentence)
        sentence = re.sub(r'www\.[^\s]+', '', sentence)
        
        # Lo·∫°i b·ªè references kh√¥ng ho√†n ch·ªânh
        sentence = re.sub(r'truy c·∫≠p v√†o th√°ng \d+.*', '', sentence)
        sentence = re.sub(r'tham kh·∫£o.*', '', sentence)
        
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát th·ª´a
        sentence = re.sub(r'[%#*]+', '', sentence)
        sentence = re.sub(r'\s+', ' ', sentence)  # Normalize whitespace
        
        # Lo·∫°i b·ªè d·∫•u c√¢u th·ª´a ·ªü cu·ªëi
        sentence = sentence.strip(' .,;:')
        
        return sentence
    
    def _create_content_summary(self, content: str, question_lower: str) -> str:
        """T·∫°o summary t·ª´ content khi kh√¥ng t√¨m ƒë∆∞·ª£c c√¢u quan tr·ªçng"""
        
        # T√¨m ƒëo·∫°n vƒÉn c√≥ ch·ª©a keywords li√™n quan
        topic_keywords = self._get_topic_keywords(question_lower)
        
        # T√°ch content th√†nh paragraphs
        paragraphs = content.split('\n')
        best_paragraph = ""
        best_score = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if len(paragraph) < 50:  # B·ªè qua ƒëo·∫°n qu√° ng·∫Øn
                continue
                
            paragraph_lower = paragraph.lower()
            score = 0
            
            # T√≠nh ƒëi·ªÉm cho paragraph
            for keyword in topic_keywords:
                score += paragraph_lower.count(keyword)
            
            if score > best_score:
                best_score = score
                best_paragraph = paragraph
        
        if best_paragraph:
            # Clean v√† truncate
            summary = self._clean_sentence(best_paragraph)
            if len(summary) > 200:
                summary = summary[:200] + "..."
            return summary
        
        # Fallback: l·∫•y ƒëo·∫°n ƒë·∫ßu
        first_part = content[:200].strip()
        return self._clean_sentence(first_part) + "..."
    
    async def query(self, 
                   question: str,
                   top_k: Optional[int] = None,
                   filter_category: Optional[str] = None,
                   include_sources: bool = True,
                   similarity_threshold: Optional[float] = None,
                   use_enhancement: bool = True) -> Dict[str, Any]:
        """API ch√≠nh ƒë·ªÉ x·ª≠ l√Ω query v·ªõi pipeline 2-stage: RAG ‚Üí LLM Enhancement"""
        try:
            start_time = time.time()
            
            if not self.is_initialized:
                await self.initialize()
            
            # Validate input
            if not question or not question.strip():
                raise ValueError("Question kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
            
            question = question.strip()
            logger.info(f"üîç Processing query with 2-stage pipeline: {question[:100]}...")
            
            # 1. Search relevant chunks
            search_results = self.search_relevant_chunks(
                question=question,
                top_k=top_k,
                filter_category=filter_category,
                similarity_threshold=similarity_threshold
            )
            
            if not search_results:
                return self._create_empty_response(question)
            
            # 2. Stage 1: Generate RAG response
            rag_response = self._generate_rag_response(question, search_results)
            
            # 3. Stage 2: LLM Enhancement (if enabled)
            if use_enhancement and self.llm_service:
                try:
                    final_response = self._enhance_response_with_llm(rag_response, question)
                except Exception as e:
                    logger.warning(f"Enhancement failed: {e}, using RAG response")
                    final_response = {
                        'original_response': rag_response['raw_response'],
                        'enhanced_response': rag_response['raw_response'],
                        'sources': rag_response['sources'],
                        'confidence': rag_response['confidence'],
                        'enhancement_applied': False,
                        'stage': 'rag_generation'
                    }
            else:
                final_response = {
                    'original_response': rag_response['raw_response'],
                    'enhanced_response': rag_response['raw_response'],
                    'sources': rag_response['sources'],
                    'confidence': rag_response['confidence'],
                    'enhancement_applied': False,
                    'stage': 'rag_generation'
                }
            
            # 4. Prepare sources information
            sources = []
            if include_sources and final_response['sources']:
                for result in final_response['sources']:
                    sources.append({
                        'filename': result['pdf_name'],
                        'display_name': self._clean_filename(result['pdf_name']),
                        'category': result['category'],
                        'content_preview': result['content'][:300] + "..." if len(result['content']) > 300 else result['content'],
                        'similarity_score': result['similarity'],
                        'content_length': result['content_length']
                    })
            
            # 5. Calculate processing time
            processing_time = int((time.time() - start_time) * 1000)
            
            # 6. Prepare final response
            response = {
                'question': question,
                'answer': final_response['enhanced_response'],
                'sources': sources,
                'total_sources': len(sources),
                'confidence': final_response['confidence'],
                'method': 'rag_llm_enhancement' if final_response['enhancement_applied'] else 'rag_generation',
                'processing_time_ms': processing_time,
                'filter_category': filter_category or 'all',
                'timestamp': datetime.now().isoformat(),
                'service_version': '2.0.0',
                'enhancement_applied': final_response['enhancement_applied'],
                'original_response': final_response['original_response'] if final_response['enhancement_applied'] else None
            }
            
            logger.info(f"‚úÖ Query processed successfully: {len(sources)} sources, {processing_time}ms, enhancement: {final_response['enhancement_applied']}")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong query unified: {e}")
            return {
                'question': question,
                'answer': f"Xin l·ªói, c√≥ l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}",
                'sources': [],
                'total_sources': 0,
                'confidence': 0.0,
                'method': 'error',
                'processing_time_ms': 0,
                'error': str(e)
            }
    
    def _create_empty_response(self, question: str) -> Dict[str, Any]:
        """T·∫°o response khi kh√¥ng c√≥ k·∫øt qu·∫£ t√¨m ki·∫øm"""
        return {
            'question': question,
            'answer': "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n trong c∆° s·ªü d·ªØ li·ªáu.",
            'sources': [],
            'total_sources': 0,
            'confidence': 0.0,
            'method': 'no_results',
            'processing_time_ms': 0,
            'filter_category': 'all',
            'timestamp': datetime.now().isoformat(),
            'service_version': '2.0.0',
            'enhancement_applied': False
        }
    
    async def get_service_stats(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ service"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # Th·ªëng k√™ theo category
            category_stats = {}
            for chunk in self.chunks_metadata:
                cat = chunk['category']
                if cat not in category_stats:
                    category_stats[cat] = {'chunks': 0, 'total_length': 0}
                category_stats[cat]['chunks'] += 1
                category_stats[cat]['total_length'] += chunk['content_length']
            
            return {
                'service_name': 'RAG Service Unified',
                'version': '1.0.0',
                'status': 'ready' if self.is_initialized else 'not_initialized',
                'initialization_time': self.initialization_time,
                'total_documents': self.total_documents,
                'total_chunks': self.total_chunks,
                'categories': category_stats,
                'device': str(self.device),
                'model_path': 'models/multilingual_e5_large',
                'default_settings': {
                    'top_k': self.default_top_k,
                    'similarity_threshold': self.default_similarity_threshold,
                    'max_answer_length': self.max_answer_length
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói get stats: {e}")
            return {'error': str(e)}

# Global instance
_rag_service_unified = None

async def get_rag_service_unified():
    """Get RAG Service Unified instance"""
    global _rag_service_unified
    if _rag_service_unified is None:
        _rag_service_unified = RAGServiceUnified()
        await _rag_service_unified.initialize()
    return _rag_service_unified
