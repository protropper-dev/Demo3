# app/core/models.py
# Model loading v√† qu·∫£n l√Ω cho backend1 (t√≠ch h·ª£p t·ª´ backend2)

import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

class ModelManager:
    """Qu·∫£n l√Ω vi·ªác load v√† s·ª≠ d·ª•ng c√°c models"""
    
    def __init__(self):
        self.llm_model = None
        self.llm_tokenizer = None
        self.embedding_model = None
        self.embedding_tokenizer = None
        self.device = None
        self._initialized = False
    
    def initialize(self):
        """Kh·ªüi t·∫°o t·∫•t c·∫£ models"""
        if self._initialized:
            return
        
        try:
            # Auto-detect device
            if torch.cuda.is_available():
                self.device = torch.device("cuda:0")
                logger.info("‚úÖ S·ª≠ d·ª•ng GPU CUDA")
            else:
                self.device = torch.device("cpu")
                logger.info("‚ö†Ô∏è  S·ª≠ d·ª•ng CPU (kh√¥ng c√≥ GPU)")
            
            # Load LLM model
            self._load_llm_model()
            
            # Load embedding model
            self._load_embedding_model()
            
            self._initialized = True
            logger.info("‚úÖ T·∫•t c·∫£ models ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi kh·ªüi t·∫°o models: {e}")
            raise
    
    def _load_llm_model(self):
        """Load LLM model"""
        try:
            logger.info(f"üîÑ ƒêang t·∫£i LLM model t·ª´: {settings.LLM_MODEL_PATH}")
            
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                settings.LLM_MODEL_PATH,
                trust_remote_code=True,  # Th√™m trust_remote_code
                padding_side="left"  # Th√™m padding_side
            )
            self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
            
            if self.device.type == "cpu":
                # C·∫•u h√¨nh cho CPU
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    settings.LLM_MODEL_PATH,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=settings.LOW_CPU_MEM_USAGE,
                    device_map="cpu"
                )
            else:
                # C·∫•u h√¨nh cho GPU v·ªõi quantization
                from transformers import BitsAndBytesConfig
                
                if settings.USE_4BIT_QUANTIZATION:
                    bnb_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_use_double_quant=settings.USE_DOUBLE_QUANTIZATION,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.float16,
                    )
                    
                    self.llm_model = AutoModelForCausalLM.from_pretrained(
                        settings.LLM_MODEL_PATH,
                        device_map="auto",  # Thay ƒë·ªïi t·ª´ "cuda:0" th√†nh "auto"
                        torch_dtype=torch.float16,
                        quantization_config=bnb_config,
                        trust_remote_code=True,  # Th√™m trust_remote_code
                    )
                else:
                    self.llm_model = AutoModelForCausalLM.from_pretrained(
                        settings.LLM_MODEL_PATH,
                        device_map="auto",  # Thay ƒë·ªïi t·ª´ "cuda:0" th√†nh "auto"
                        torch_dtype=torch.float16,
                        trust_remote_code=True,  # Th√™m trust_remote_code
                    )
            
            # Ki·ªÉm tra xem model c√≥ ph·∫£i l√† quantized kh√¥ng tr∆∞·ªõc khi g·ªçi .eval()
            if hasattr(self.llm_model, 'config') and hasattr(self.llm_model.config, 'quantization_config'):
                logger.info("‚úÖ LLM model (quantized) ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
            else:
                self.llm_model.eval()
                logger.info("‚úÖ LLM model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫£i LLM model: {e}")
            self.llm_model = None
            self.llm_tokenizer = None
    
    def _load_embedding_model(self):
        """Load embedding model"""
        try:
            logger.info(f"üîÑ ƒêang t·∫£i Embedding model t·ª´: {settings.EMBEDDING_MODEL_PATH}")
            
            self.embedding_model = SentenceTransformer(settings.EMBEDDING_MODEL_PATH)
            self.embedding_tokenizer = AutoTokenizer.from_pretrained(settings.EMBEDDING_MODEL_PATH)
            
            logger.info("‚úÖ Embedding model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫£i Embedding model: {e}")
            self.embedding_model = None
            self.embedding_tokenizer = None
    
    def get_models_status(self):
        """L·∫•y tr·∫°ng th√°i c·ªßa c√°c models"""
        return {
            "llm_model_loaded": self.llm_model is not None,
            "llm_tokenizer_loaded": self.llm_tokenizer is not None,
            "embedding_model_loaded": self.embedding_model is not None,
            "embedding_tokenizer_loaded": self.embedding_tokenizer is not None,
            "device": str(self.device) if self.device else "unknown",
            "initialized": self._initialized,
            "cuda_available": torch.cuda.is_available(),
            "cuda_device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        }
    
    def is_ready(self):
        """Ki·ªÉm tra xem t·∫•t c·∫£ models ƒë√£ s·∫µn s√†ng ch∆∞a"""
        return (
            self.llm_model is not None
            and self.llm_tokenizer is not None
            and self.embedding_model is not None
            and self.embedding_tokenizer is not None
        )

# Global model manager instance
model_manager = ModelManager()

def get_model_manager():
    """L·∫•y model manager instance"""
    return model_manager
