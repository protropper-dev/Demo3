"""
C·∫•u h√¨nh linh ho·∫°t cho backend2
ƒêi·ªÅu ch·ªânh c√°c th√¥ng s·ªë n√†y ph√π h·ª£p v·ªõi m√°y c·ªßa b·∫°n
"""

import torch
from pathlib import Path

# ==================== TH√îNG S·ªê M√ÅY ====================
class DeviceConfig:
    """C·∫•u h√¨nh thi·∫øt b·ªã"""
    
    # T·ª± ƒë·ªông ph√°t hi·ªán GPU/CPU
    USE_GPU = torch.cuda.is_available()
    DEVICE = "cuda:0" if USE_GPU else "cpu"
    
    # C·∫•u h√¨nh GPU
    GPU_MEMORY_FRACTION = 0.8  # S·ª≠ d·ª•ng 80% GPU memory
    GPU_DEVICE_ID = 0
    
    # C·∫•u h√¨nh CPU
    CPU_THREADS = 4  # S·ªë thread cho CPU
    
    @classmethod
    def get_device_info(cls):
        """L·∫•y th√¥ng tin thi·∫øt b·ªã"""
        info = {
            "device": cls.DEVICE,
            "use_gpu": cls.USE_GPU,
            "gpu_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        if cls.USE_GPU and torch.cuda.is_available():
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["gpu_memory"] = torch.cuda.get_device_properties(0).total_memory
        
        return info

# ==================== TH√îNG S·ªê M√î H√åNH ====================
class ModelConfig:
    """C·∫•u h√¨nh m√¥ h√¨nh"""
    
    # LLM Model
    LLM_MAX_TOKENS = 512  # TƒÉng t·ª´ 256 ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi d√†i h∆°n
    LLM_TEMPERATURE = 0.7
    LLM_TOP_K = 50
    LLM_TOP_P = 0.95
    
    # Embedding
    EMBEDDING_TOP_K = 5  # TƒÉng s·ªë chunks ƒë·ªÉ c√≥ context t·ªët h∆°n
    EMBEDDING_MAX_TOKENS_PER_CHUNK = 4096  # Gi·∫£m ƒë·ªÉ ph√π h·ª£p v·ªõi context window
    
    # Quantization cho GPU
    USE_4BIT_QUANTIZATION = True
    USE_DOUBLE_QUANTIZATION = True
    
    # Memory optimization
    LOW_CPU_MEM_USAGE = True
    USE_CACHE = True

# ==================== TH√îNG S·ªê SERVER ====================
class ServerConfig:
    """C·∫•u h√¨nh server"""
    
    HOST = "0.0.0.0"
    PORT = 8001
    WORKERS = 1  # FastAPI v·ªõi reload=True ch·ªâ n√™n d√πng 1 worker
    
    # CORS
    ALLOWED_ORIGINS = ["*"]
    ALLOW_CREDENTIALS = True
    
    # Timeout
    REQUEST_TIMEOUT = 300  # 5 ph√∫t
    STREAM_TIMEOUT = 60    # 1 ph√∫t

# ==================== TH√îNG S·ªê FILE ====================
class FileConfig:
    """C·∫•u h√¨nh file"""
    
    # ƒê·ªãnh d·∫°ng file ƒë∆∞·ª£c ph√©p
    ALLOWED_EXTENSIONS = {".pdf", ".doc", ".docx", ".txt"}
    
    # K√≠ch th∆∞·ªõc file t·ªëi ƒëa (MB)
    MAX_FILE_SIZE = 50
    
    # Th∆∞ m·ª•c upload
    UPLOAD_FOLDER = "documents/upload"
    
    # Chunk size cho file processing
    CHUNK_SIZE = 1024 * 1024  # 1MB

# ==================== TH√îNG S·ªê LOGGING ====================
class LogConfig:
    """C·∫•u h√¨nh logging"""
    
    LOG_LEVEL = "INFO"
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    LOG_FILE = "backend2.log"
    
    # Log c√°c th√¥ng tin quan tr·ªçng
    LOG_MODEL_LOADING = True
    LOG_REQUEST_RESPONSE = True
    LOG_ERROR_DETAILS = True

# ==================== H√ÄM TI·ªÜN √çCH ====================
def get_optimal_config():
    """L·∫•y c·∫•u h√¨nh t·ªëi ∆∞u d·ª±a tr√™n m√°y hi·ªán t·∫°i"""
    
    device_info = DeviceConfig.get_device_info()
    
    config = {
        "device": DeviceConfig.DEVICE,
        "use_gpu": DeviceConfig.USE_GPU,
        "model_config": {
            "max_tokens": ModelConfig.LLM_MAX_TOKENS,
            "temperature": ModelConfig.LLM_TEMPERATURE,
            "top_k": ModelConfig.LLM_TOP_K,
            "top_p": ModelConfig.LLM_TOP_P,
            "embedding_top_k": ModelConfig.EMBEDDING_TOP_K,
        },
        "server_config": {
            "host": ServerConfig.HOST,
            "port": ServerConfig.PORT,
            "workers": ServerConfig.WORKERS,
        },
        "file_config": {
            "allowed_extensions": list(FileConfig.ALLOWED_EXTENSIONS),
            "max_file_size": FileConfig.MAX_FILE_SIZE,
        }
    }
    
    return config

def print_config_summary():
    """In t√≥m t·∫Øt c·∫•u h√¨nh"""
    print("üîß Backend2 Configuration Summary:")
    print("=" * 50)
    
    device_info = DeviceConfig.get_device_info()
    print(f"üñ•Ô∏è  Device: {device_info['device']}")
    print(f"üéÆ GPU Available: {device_info['gpu_available']}")
    
    if device_info['gpu_available']:
        print(f"üéÆ GPU Count: {device_info['gpu_count']}")
        print(f"üéÆ GPU Name: {device_info.get('gpu_name', 'Unknown')}")
    
    print(f"‚öôÔ∏è  Max Tokens: {ModelConfig.LLM_MAX_TOKENS}")
    print(f"üå°Ô∏è  Temperature: {ModelConfig.LLM_TEMPERATURE}")
    print(f"üì° Server: {ServerConfig.HOST}:{ServerConfig.PORT}")
    print(f"üìÅ Upload Folder: {FileConfig.UPLOAD_FOLDER}")
    print("=" * 50)

if __name__ == "__main__":
    print_config_summary()
