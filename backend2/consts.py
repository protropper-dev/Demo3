import settings
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
from config import DeviceConfig, ModelConfig

# S·ª≠ d·ª•ng c·∫•u h√¨nh t·ª´ config
device = torch.device(DeviceConfig.DEVICE)
print(f"‚úÖ S·ª≠ d·ª•ng thi·∫øt b·ªã: {DeviceConfig.DEVICE}")

# Load LLM model v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u cho m√°y hi·ªán t·∫°i
llm_model_path = str(settings.MODEL)
print(f"üîÑ ƒêang t·∫£i LLM model t·ª´: {llm_model_path}")

try:
    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)
    llm_tokenizer.pad_token = llm_tokenizer.eos_token  # Set padding token
    
    # C·∫•u h√¨nh quantization cho CPU ho·∫∑c GPU y·∫øu
    if device.type == "cpu":
        # C·∫•u h√¨nh cho CPU
        llm_model = AutoModelForCausalLM.from_pretrained(
            llm_model_path,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=ModelConfig.LOW_CPU_MEM_USAGE,
            device_map="cpu"
        )
    else:
        # C·∫•u h√¨nh cho GPU v·ªõi quantization
        from transformers import BitsAndBytesConfig
        
        if ModelConfig.USE_4BIT_QUANTIZATION:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=ModelConfig.USE_DOUBLE_QUANTIZATION,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )
            
            llm_model = AutoModelForCausalLM.from_pretrained(
                llm_model_path,
                device_map="cuda:0",
                torch_dtype=torch.float16,
                quantization_config=bnb_config,
            )
        else:
            llm_model = AutoModelForCausalLM.from_pretrained(
                llm_model_path,
                device_map="cuda:0",
                torch_dtype=torch.float16,
            )
    
    llm_model.eval()  # Chuy·ªÉn model sang ch·∫ø ƒë·ªô ƒë√°nh gi√°
    print("‚úÖ LLM model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
    
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i LLM model: {e}")
    llm_model = None
    llm_tokenizer = None

# Load embedding model
embedding_model_path = str(settings.EMBEDDING_MODEL_FOLDER)
print(f"üîÑ ƒêang t·∫£i Embedding model t·ª´: {embedding_model_path}")

try:
    embedding_model = SentenceTransformer(embedding_model_path)
    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_path)
    print("‚úÖ Embedding model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i Embedding model: {e}")
    embedding_model = None
    embedding_tokenizer = None

# Ki·ªÉm tra c√°c model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng
models_loaded = (
    embedding_model is not None
    and embedding_tokenizer is not None
    and llm_model is not None
    and llm_tokenizer is not None
)

if not models_loaded:
    print("‚ö†Ô∏è  C·∫£nh b√°o: M·ªôt s·ªë m√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")
    print("   Backend v·∫´n s·∫Ω kh·ªüi ƒë·ªông nh∆∞ng m·ªôt s·ªë ch·ª©c nƒÉng c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông.")
    print("   Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh trong settings.py")
else:
    print("‚úÖ T·∫•t c·∫£ m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
