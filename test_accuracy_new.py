#!/usr/bin/env python3
"""
Accuracy Testing Script cho Há»‡ thá»‘ng Trá»£ lÃ½ áº¢o An toÃ n ThÃ´ng tin
ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c theo yÃªu cáº§u 4.3.4.1, 4.3.4.2, 4.3.4.3
"""

import asyncio
import aiohttp
import json
import time
import csv
import statistics
import re
from datetime import datetime
from typing import List, Dict, Any, Tuple
import logging
from dataclasses import dataclass

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('accuracy_test_results.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ExpertEvaluationCase:
    """Test case cho Ä‘Ã¡nh giÃ¡ bá»Ÿi chuyÃªn gia (4.3.4.1)"""
    question: str
    language: str  # "vietnamese", "english", "mixed"
    category: str  # luat, english, vietnamese, technical
    expected_keywords: List[str]
    expert_notes: str

@dataclass
class SearchAccuracyCase:
    """Test case cho Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m (4.3.4.2)"""
    question: str
    expected_answer: str
    expected_sources: List[str]  # CÃ¡c nguá»“n tÃ i liá»‡u mong Ä‘á»£i
    language: str
    category: str

@dataclass
class MultilingualCase:
    """Test case cho thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯ (4.3.4.3)"""
    question: str
    language: str  # "vietnamese", "english", "mixed"
    expected_response_language: str
    category: str

class AccuracyTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.api_endpoint = f"{base_url}/api/v1/rag/query"
        self.session = None
        
        # Táº¡o bá»™ test cases cho tá»«ng loáº¡i Ä‘Ã¡nh giÃ¡
        self.expert_evaluation_cases = self._create_expert_evaluation_cases()
        self.search_accuracy_cases = self._create_search_accuracy_cases()
        self.multilingual_cases = self._create_multilingual_cases()
    
    def _create_expert_evaluation_cases(self) -> List[ExpertEvaluationCase]:
        """Táº¡o 50 cÃ¢u há»i chuáº©n cho Ä‘Ã¡nh giÃ¡ bá»Ÿi chuyÃªn gia (4.3.4.1)"""
        # ÄÃ¢y lÃ  phiÃªn báº£n rÃºt gá»n, trong thá»±c táº¿ sáº½ cÃ³ 50 cÃ¢u há»i Ä‘áº§y Ä‘á»§
        return [
            ExpertEvaluationCase(
                question="An toÃ n thÃ´ng tin lÃ  gÃ¬?",
                language="vietnamese",
                category="general",
                expected_keywords=["an toÃ n", "thÃ´ng tin", "báº£o vá»‡", "bÃ­ máº­t", "toÃ n váº¹n", "sáºµn sÃ ng"],
                expert_notes="CÃ¢u há»i cÆ¡ báº£n, cáº§n giáº£i thÃ­ch khÃ¡i niá»‡m CIA triad"
            ),
            ExpertEvaluationCase(
                question="What is information security?",
                language="english",
                category="general",
                expected_keywords=["information", "security", "protection", "confidentiality", "integrity", "availability"],
                expert_notes="Basic concept in English, should explain CIA triad"
            ),
            # ... thÃªm 48 cÃ¢u há»i khÃ¡c
        ]
    
    def _create_search_accuracy_cases(self) -> List[SearchAccuracyCase]:
        """Táº¡o 30 cÃ¢u há»i vá»›i cÃ¢u tráº£ lá»i chuáº©n Ä‘Ã£ biáº¿t (4.3.4.2)"""
        return [
            SearchAccuracyCase(
                question="Luáº­t An toÃ n thÃ´ng tin sá»‘ 86/2015/QH13 cÃ³ hiá»‡u lá»±c tá»« khi nÃ o?",
                expected_answer="Luáº­t An toÃ n thÃ´ng tin sá»‘ 86/2015/QH13 cÃ³ hiá»‡u lá»±c tá»« ngÃ y 01/7/2016",
                expected_sources=["Luat-86-2015-QH13.pdf"],
                language="vietnamese",
                category="luat"
            ),
            # ... thÃªm 29 cÃ¢u há»i khÃ¡c
        ]
    
    def _create_multilingual_cases(self) -> List[MultilingualCase]:
        """Táº¡o test cases cho thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯ (4.3.4.3)"""
        return [
            MultilingualCase(
                question="An toÃ n thÃ´ng tin lÃ  gÃ¬?",
                language="vietnamese",
                expected_response_language="vietnamese",
                category="general"
            ),
            MultilingualCase(
                question="What is information security?",
                language="english",
                expected_response_language="english",
                category="general"
            ),
            # ... thÃªm 38 cÃ¢u há»i khÃ¡c
        ]
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _evaluate_by_expert_criteria(self, test_case: ExpertEvaluationCase, answer: str, sources: List[Dict]) -> Dict[str, Any]:
        """ÄÃ¡nh giÃ¡ theo 5 tiÃªu chÃ­ chuyÃªn gia (thang Ä‘iá»ƒm 1-5)"""
        # 1. Äá»™ chÃ­nh xÃ¡c thÃ´ng tin (Accuracy) - 1-5
        accuracy_score = self._calculate_accuracy_score(test_case, answer)
        
        # 2. TÃ­nh Ä‘áº§y Ä‘á»§ (Completeness) - 1-5
        completeness_score = self._calculate_completeness_score(test_case, answer)
        
        # 3. TÃ­nh liÃªn quan (Relevance) - 1-5
        relevance_score = self._calculate_relevance_score(test_case, answer)
        
        # 4. TÃ­nh dá»… hiá»ƒu (Clarity) - 1-5
        clarity_score = self._calculate_clarity_score(answer, test_case.language)
        
        # 5. TÃ­nh cáº­p nháº­t (Currency) - 1-5
        currency_score = self._calculate_currency_score(test_case, sources)
        
        overall_score = (accuracy_score + completeness_score + relevance_score + clarity_score + currency_score) / 5
        
        return {
            "accuracy": accuracy_score,
            "completeness": completeness_score,
            "relevance": relevance_score,
            "clarity": clarity_score,
            "currency": currency_score,
            "overall_score": overall_score
        }
    
    def _calculate_accuracy_score(self, test_case: ExpertEvaluationCase, answer: str) -> float:
        """TÃ­nh Ä‘iá»ƒm Ä‘á»™ chÃ­nh xÃ¡c thÃ´ng tin"""
        answer_lower = answer.lower()
        
        # Kiá»ƒm tra tá»« khÃ³a quan trá»ng
        keyword_count = 0
        for keyword in test_case.expected_keywords:
            if keyword.lower() in answer_lower:
                keyword_count += 1
        
        keyword_score = keyword_count / len(test_case.expected_keywords) if test_case.expected_keywords else 0
        
        # Chuyá»ƒn Ä‘á»•i thÃ nh thang Ä‘iá»ƒm 1-5
        if keyword_score >= 0.8:
            return 5.0
        elif keyword_score >= 0.6:
            return 4.0
        elif keyword_score >= 0.4:
            return 3.0
        elif keyword_score >= 0.2:
            return 2.0
        else:
            return 1.0
    
    def _calculate_completeness_score(self, test_case: ExpertEvaluationCase, answer: str) -> float:
        """TÃ­nh Ä‘iá»ƒm tÃ­nh Ä‘áº§y Ä‘á»§"""
        answer_length = len(answer)
        
        # Äiá»u chá»‰nh theo loáº¡i cÃ¢u há»i
        if test_case.category == "general":
            expected_length = 100
        elif test_case.category == "technical":
            expected_length = 200
        elif test_case.category == "luat":
            expected_length = 250
        else:
            expected_length = 150
        
        length_ratio = answer_length / expected_length
        
        if length_ratio >= 1.0:
            return 5.0
        elif length_ratio >= 0.8:
            return 4.0
        elif length_ratio >= 0.6:
            return 3.0
        elif length_ratio >= 0.4:
            return 2.0
        else:
            return 1.0
    
    def _calculate_relevance_score(self, test_case: ExpertEvaluationCase, answer: str) -> float:
        """TÃ­nh Ä‘iá»ƒm tÃ­nh liÃªn quan"""
        answer_lower = answer.lower()
        question_lower = test_case.question.lower()
        
        # TrÃ­ch xuáº¥t tá»« khÃ³a tá»« cÃ¢u há»i
        question_words = set(re.findall(r'\b\w+\b', question_lower))
        answer_words = set(re.findall(r'\b\w+\b', answer_lower))
        
        # TÃ­nh Ä‘á»™ trÃ¹ng láº·p
        overlap = len(question_words & answer_words)
        relevance_ratio = overlap / len(question_words) if question_words else 0
        
        if relevance_ratio >= 0.7:
            return 5.0
        elif relevance_ratio >= 0.5:
            return 4.0
        elif relevance_ratio >= 0.3:
            return 3.0
        elif relevance_ratio >= 0.1:
            return 2.0
        else:
            return 1.0
    
    def _calculate_clarity_score(self, answer: str, language: str) -> float:
        """TÃ­nh Ä‘iá»ƒm tÃ­nh dá»… hiá»ƒu"""
        if not answer:
            return 1.0
        
        # Kiá»ƒm tra cáº¥u trÃºc cÃ¢u
        sentences = re.split(r'[.!?]+', answer)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) < 2:
            return 2.0
        
        # Kiá»ƒm tra Ä‘á»™ dÃ i cÃ¢u trung bÃ¬nh
        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
        
        # Äiá»u chá»‰nh theo ngÃ´n ngá»¯
        if language == "vietnamese":
            if 20 <= avg_sentence_length <= 80:
                return 5.0
            elif 10 <= avg_sentence_length <= 120:
                return 4.0
            else:
                return 3.0
        else:  # english
            if 15 <= avg_sentence_length <= 60:
                return 5.0
            elif 10 <= avg_sentence_length <= 90:
                return 4.0
            else:
                return 3.0
    
    def _calculate_currency_score(self, test_case: ExpertEvaluationCase, sources: List[Dict]) -> float:
        """TÃ­nh Ä‘iá»ƒm tÃ­nh cáº­p nháº­t"""
        if not sources:
            return 2.0
        
        # Kiá»ƒm tra nguá»“n tÃ i liá»‡u cÃ³ gáº§n Ä‘Ã¢y khÃ´ng
        recent_sources = 0
        for source in sources:
            filename = source.get("filename", "").lower()
            # Kiá»ƒm tra nÄƒm trong tÃªn file
            if "2024" in filename or "2023" in filename or "2022" in filename:
                recent_sources += 1
        
        currency_ratio = recent_sources / len(sources)
        
        if currency_ratio >= 0.8:
            return 5.0
        elif currency_ratio >= 0.6:
            return 4.0
        elif currency_ratio >= 0.4:
            return 3.0
        elif currency_ratio >= 0.2:
            return 2.0
        else:
            return 1.0
    
    def _calculate_search_metrics(self, test_case: SearchAccuracyCase, answer: str, sources: List[Dict]) -> Dict[str, Any]:
        """TÃ­nh toÃ¡n Precision, Recall, F1-Score"""
        # Precision: Sá»‘ cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c / Tá»•ng sá»‘ cÃ¢u tráº£ lá»i
        precision = self._calculate_precision(test_case, answer)
        
        # Recall: Sá»‘ thÃ´ng tin chÃ­nh xÃ¡c Ä‘Æ°á»£c tÃ¬m tháº¥y / Tá»•ng sá»‘ thÃ´ng tin chÃ­nh xÃ¡c
        recall = self._calculate_recall(test_case, answer)
        
        # F1-Score: Harmonic mean cá»§a Precision vÃ  Recall
        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score
        }
    
    def _calculate_precision(self, test_case: SearchAccuracyCase, answer: str) -> float:
        """TÃ­nh Precision"""
        # ÄÆ¡n giáº£n hÃ³a: kiá»ƒm tra xem cÃ¢u tráº£ lá»i cÃ³ chá»©a thÃ´ng tin chÃ­nh xÃ¡c khÃ´ng
        expected_lower = test_case.expected_answer.lower()
        answer_lower = answer.lower()
        
        # TÃ¡ch tá»« vÃ  so sÃ¡nh
        expected_words = set(re.findall(r'\b\w+\b', expected_lower))
        answer_words = set(re.findall(r'\b\w+\b', answer_lower))
        
        # Äáº¿m tá»« khÃ³a chÃ­nh
        key_words = {"luáº­t", "nghá»‹ Ä‘á»‹nh", "thÃ´ng tÆ°", "quyáº¿t Ä‘á»‹nh", "iso", "nist", "tcvn", "ngÃ y", "thÃ¡ng", "nÄƒm"}
        expected_key_words = expected_words & key_words
        answer_key_words = answer_words & key_words
        
        if not expected_key_words:
            return 1.0
        
        precision = len(expected_key_words & answer_key_words) / len(expected_key_words)
        return precision
    
    def _calculate_recall(self, test_case: SearchAccuracyCase, answer: str) -> float:
        """TÃ­nh Recall"""
        expected_lower = test_case.expected_answer.lower()
        answer_lower = answer.lower()
        
        # TÃ¡ch tá»« vÃ  so sÃ¡nh
        expected_words = set(re.findall(r'\b\w+\b', expected_lower))
        answer_words = set(re.findall(r'\b\w+\b', answer_lower))
        
        if not expected_words:
            return 1.0
        
        # TÃ­nh tá»· lá»‡ tá»« khÃ³a Ä‘Æ°á»£c tÃ¬m tháº¥y
        recall = len(expected_words & answer_words) / len(expected_words)
        return recall
    
    def _evaluate_multilingual_accuracy(self, test_case: MultilingualCase, answer: str) -> float:
        """ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cho thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯"""
        # Kiá»ƒm tra ngÃ´n ngá»¯ pháº£n há»“i cÃ³ phÃ¹ há»£p khÃ´ng
        language_match = self._check_language_match(test_case.expected_response_language, answer)
        
        # Kiá»ƒm tra cháº¥t lÆ°á»£ng ná»™i dung
        content_quality = self._check_content_quality(answer)
        
        return (language_match + content_quality) / 2
    
    def _check_language_match(self, expected_language: str, answer: str) -> float:
        """Kiá»ƒm tra ngÃ´n ngá»¯ pháº£n há»“i cÃ³ phÃ¹ há»£p khÃ´ng"""
        if expected_language == "vietnamese":
            # Kiá»ƒm tra tá»« tiáº¿ng Viá»‡t
            vietnamese_words = ["lÃ ", "cá»§a", "vÃ ", "vá»›i", "trong", "cho", "Ä‘áº¿n", "tá»«", "cÃ³", "Ä‘Æ°á»£c", "nÃ y", "Ä‘Ã³", "cÃ¡c", "má»™t", "nhÆ°", "vá»", "sáº½", "Ä‘Ã£", "Ä‘ang", "bá»‹"]
            vietnamese_count = sum(1 for word in vietnamese_words if word in answer.lower())
            return min(vietnamese_count / 5, 1.0)
        elif expected_language == "english":
            # Kiá»ƒm tra tá»« tiáº¿ng Anh
            english_words = ["the", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had"]
            english_count = sum(1 for word in english_words if word.lower() in answer.lower())
            return min(english_count / 5, 1.0)
        else:
            return 0.5
    
    def _check_content_quality(self, answer: str) -> float:
        """Kiá»ƒm tra cháº¥t lÆ°á»£ng ná»™i dung"""
        if len(answer) < 50:
            return 0.3
        elif len(answer) < 100:
            return 0.6
        elif len(answer) < 200:
            return 0.8
        else:
            return 1.0
    
    def _calculate_expert_evaluation_stats(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """TÃ­nh toÃ¡n thá»‘ng kÃª Ä‘Ã¡nh giÃ¡ chuyÃªn gia"""
        all_scores = []
        accuracy_scores = []
        completeness_scores = []
        relevance_scores = []
        clarity_scores = []
        currency_scores = []
        
        for result in results:
            eval_data = result.get("expert_evaluation", {})
            if eval_data:
                all_scores.append(eval_data.get("overall_score", 0))
                accuracy_scores.append(eval_data.get("accuracy", 0))
                completeness_scores.append(eval_data.get("completeness", 0))
                relevance_scores.append(eval_data.get("relevance", 0))
                clarity_scores.append(eval_data.get("clarity", 0))
                currency_scores.append(eval_data.get("currency", 0))
        
        def calculate_stats(scores):
            if not scores:
                return {"mean": 0, "std_dev": 0}
            return {
                "mean": statistics.mean(scores),
                "std_dev": statistics.stdev(scores) if len(scores) > 1 else 0
            }
        
        return {
            "overall": calculate_stats(all_scores),
            "accuracy": calculate_stats(accuracy_scores),
            "completeness": calculate_stats(completeness_scores),
            "relevance": calculate_stats(relevance_scores),
            "clarity": calculate_stats(clarity_scores),
            "currency": calculate_stats(currency_scores)
        }
    
    def _calculate_overall_search_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """TÃ­nh toÃ¡n thá»‘ng kÃª tÃ¬m kiáº¿m tá»•ng há»£p"""
        all_precision = []
        all_recall = []
        all_f1_scores = []
        
        for result in results:
            metrics = result.get("search_metrics", {})
            if metrics:
                all_precision.append(metrics.get("precision", 0))
                all_recall.append(metrics.get("recall", 0))
                all_f1_scores.append(metrics.get("f1_score", 0))
        
        return {
            "precision": {
                "mean": statistics.mean(all_precision) if all_precision else 0,
                "std_dev": statistics.stdev(all_precision) if len(all_precision) > 1 else 0
            },
            "recall": {
                "mean": statistics.mean(all_recall) if all_recall else 0,
                "std_dev": statistics.stdev(all_recall) if len(all_recall) > 1 else 0
            },
            "f1_score": {
                "mean": statistics.mean(all_f1_scores) if all_f1_scores else 0,
                "std_dev": statistics.stdev(all_f1_scores) if len(all_f1_scores) > 1 else 0
            }
        }
    
    def _calculate_multilingual_stats(self, vietnamese_results: List[Dict], english_results: List[Dict], mixed_results: List[Dict]) -> Dict[str, Any]:
        """TÃ­nh toÃ¡n thá»‘ng kÃª Ä‘a ngÃ´n ngá»¯"""
        def calculate_language_stats(results):
            if not results:
                return {"count": 0, "avg_accuracy": 0, "avg_response_time": 0}
            
            accuracies = [r.get("accuracy", 0) for r in results if r.get("success", False)]
            response_times = [r.get("response_time_ms", 0) for r in results if r.get("success", False)]
            
            return {
                "count": len(results),
                "avg_accuracy": statistics.mean(accuracies) if accuracies else 0,
                "avg_response_time": statistics.mean(response_times) if response_times else 0
            }
        
        return {
            "vietnamese": calculate_language_stats(vietnamese_results),
            "english": calculate_language_stats(english_results),
            "mixed": calculate_language_stats(mixed_results)
        }
    
    def save_expert_evaluation_results(self, results: Dict[str, Any]):
        """LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ chuyÃªn gia"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"expert_evaluation_test_{timestamp}.json"
        
        output = {
            "test_type": "expert_evaluation_4_3_4_1",
            "timestamp": timestamp,
            "results": results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        # Táº¡o CSV cho báº£ng káº¿t quáº£
        csv_filename = f"expert_evaluation_table_{timestamp}.csv"
        self._create_expert_evaluation_table_csv(results, csv_filename)
        
        logger.info(f"ğŸ“Š Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ chuyÃªn gia Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {filename}")
        logger.info(f"ğŸ“Š Báº£ng káº¿t quáº£ CSV Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {csv_filename}")
        return filename, csv_filename
    
    def save_search_accuracy_results(self, results: Dict[str, Any]):
        """LÆ°u káº¿t quáº£ Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"search_accuracy_test_{timestamp}.json"
        
        output = {
            "test_type": "search_accuracy_4_3_4_2",
            "timestamp": timestamp,
            "results": results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        # Táº¡o CSV cho báº£ng káº¿t quáº£
        csv_filename = f"search_accuracy_table_{timestamp}.csv"
        self._create_search_accuracy_table_csv(results, csv_filename)
        
        logger.info(f"ğŸ“Š Káº¿t quáº£ Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {filename}")
        logger.info(f"ğŸ“Š Báº£ng káº¿t quáº£ CSV Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {csv_filename}")
        return filename, csv_filename
    
    def save_multilingual_results(self, results: Dict[str, Any]):
        """LÆ°u káº¿t quáº£ thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"multilingual_test_{timestamp}.json"
        
        output = {
            "test_type": "multilingual_4_3_4_3",
            "timestamp": timestamp,
            "results": results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        # Táº¡o CSV cho báº£ng káº¿t quáº£
        csv_filename = f"multilingual_table_{timestamp}.csv"
        self._create_multilingual_table_csv(results, csv_filename)
        
        logger.info(f"ğŸ“Š Káº¿t quáº£ thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {filename}")
        logger.info(f"ğŸ“Š Báº£ng káº¿t quáº£ CSV Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {csv_filename}")
        return filename, csv_filename
    
    def _create_expert_evaluation_table_csv(self, results: Dict[str, Any], filename: str):
        """Táº¡o báº£ng CSV cho káº¿t quáº£ Ä‘Ã¡nh giÃ¡ chuyÃªn gia"""
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # Header
            writer.writerow(['TiÃªu chÃ­', 'Äiá»ƒm trung bÃ¬nh', 'Äá»™ lá»‡ch chuáº©n', 'Ghi chÃº'])
            
            if 'expert_evaluation_stats' in results:
                stats = results['expert_evaluation_stats']
                
                writer.writerow([
                    'Äá»™ chÃ­nh xÃ¡c thÃ´ng tin (Accuracy)',
                    f"{stats['accuracy']['mean']:.2f}",
                    f"{stats['accuracy']['std_dev']:.2f}",
                    'Thang Ä‘iá»ƒm 1-5'
                ])
                
                writer.writerow([
                    'TÃ­nh Ä‘áº§y Ä‘á»§ (Completeness)',
                    f"{stats['completeness']['mean']:.2f}",
                    f"{stats['completeness']['std_dev']:.2f}",
                    'Thang Ä‘iá»ƒm 1-5'
                ])
                
                writer.writerow([
                    'TÃ­nh liÃªn quan (Relevance)',
                    f"{stats['relevance']['mean']:.2f}",
                    f"{stats['relevance']['std_dev']:.2f}",
                    'Thang Ä‘iá»ƒm 1-5'
                ])
                
                writer.writerow([
                    'TÃ­nh dá»… hiá»ƒu (Clarity)',
                    f"{stats['clarity']['mean']:.2f}",
                    f"{stats['clarity']['std_dev']:.2f}",
                    'Thang Ä‘iá»ƒm 1-5'
                ])
                
                writer.writerow([
                    'TÃ­nh cáº­p nháº­t (Currency)',
                    f"{stats['currency']['mean']:.2f}",
                    f"{stats['currency']['std_dev']:.2f}",
                    'Thang Ä‘iá»ƒm 1-5'
                ])
    
    def _create_search_accuracy_table_csv(self, results: Dict[str, Any], filename: str):
        """Táº¡o báº£ng CSV cho káº¿t quáº£ Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m"""
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # Header
            writer.writerow(['Metric', 'GiÃ¡ trá»‹', 'Ghi chÃº'])
            
            if 'overall_search_metrics' in results:
                metrics = results['overall_search_metrics']
                
                writer.writerow([
                    'Precision',
                    f"{metrics['precision']['mean']:.3f}",
                    'Äá»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m'
                ])
                
                writer.writerow([
                    'Recall',
                    f"{metrics['recall']['mean']:.3f}",
                    'Äá»™ bao phá»§ tÃ¬m kiáº¿m'
                ])
                
                writer.writerow([
                    'F1-Score',
                    f"{metrics['f1_score']['mean']:.3f}",
                    'Trung bÃ¬nh Ä‘iá»u hÃ²a cá»§a Precision vÃ  Recall'
                ])
    
    def _create_multilingual_table_csv(self, results: Dict[str, Any], filename: str):
        """Táº¡o báº£ng CSV cho káº¿t quáº£ thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯"""
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # Header
            writer.writerow(['NgÃ´n ngá»¯', 'Sá»‘ cÃ¢u há»i', 'Äá»™ chÃ­nh xÃ¡c', 'Thá»i gian pháº£n há»“i (s)'])
            
            if 'multilingual_stats' in results:
                stats = results['multilingual_stats']
                
                writer.writerow([
                    'Tiáº¿ng Viá»‡t',
                    stats['vietnamese']['count'],
                    f"{stats['vietnamese']['avg_accuracy']:.3f}",
                    f"{stats['vietnamese']['avg_response_time']/1000:.3f}"
                ])
                
                writer.writerow([
                    'Tiáº¿ng Anh',
                    stats['english']['count'],
                    f"{stats['english']['avg_accuracy']:.3f}",
                    f"{stats['english']['avg_response_time']/1000:.3f}"
                ])
                
                writer.writerow([
                    'Há»—n há»£p',
                    stats['mixed']['count'],
                    f"{stats['mixed']['avg_accuracy']:.3f}",
                    f"{stats['mixed']['avg_response_time']/1000:.3f}"
                ])
    
    def print_expert_evaluation_summary(self, results: Dict[str, Any]):
        """In tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡ chuyÃªn gia"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Káº¾T QUáº¢ THá»¬ NGHIá»†M Äá»˜ CHÃNH XÃC CÃ‚U TRáº¢ Lá»œI (4.3.4.1)")
        print(f"{'='*80}")
        
        if "error" in results:
            print(f"âŒ Lá»—i: {results['error']}")
            return
        
        print(f"ğŸ“Š Tá»•ng quan:")
        print(f"   â€¢ Tá»•ng cÃ¢u há»i test: {results['total_questions']}")
        print(f"   â€¢ CÃ¢u há»i thÃ nh cÃ´ng: {results['successful_questions']}")
        print(f"   â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {results['success_rate']:.1f}%")
        
        if "expert_evaluation_stats" in results:
            stats = results["expert_evaluation_stats"]
            print(f"\nğŸ¯ ÄÃNH GIÃ THEO 5 TIÃŠU CHÃ (Thang Ä‘iá»ƒm 1-5):")
            print("-" * 60)
            
            criteria = [
                ("Äá»™ chÃ­nh xÃ¡c thÃ´ng tin (Accuracy)", stats["accuracy"]),
                ("TÃ­nh Ä‘áº§y Ä‘á»§ (Completeness)", stats["completeness"]),
                ("TÃ­nh liÃªn quan (Relevance)", stats["relevance"]),
                ("TÃ­nh dá»… hiá»ƒu (Clarity)", stats["clarity"]),
                ("TÃ­nh cáº­p nháº­t (Currency)", stats["currency"])
            ]
            
            for criterion_name, criterion_stats in criteria:
                print(f"{criterion_name:30} | TB: {criterion_stats['mean']:4.2f} | ÄLC: {criterion_stats['std_dev']:4.2f}")
            
            print(f"\nğŸ“Š Äiá»ƒm tá»•ng thá»ƒ: {stats['overall']['mean']:.2f}/5.0 (ÄLC: {stats['overall']['std_dev']:.2f})")
    
    def print_search_accuracy_summary(self, results: Dict[str, Any]):
        """In tÃ³m táº¯t káº¿t quáº£ Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m"""
        print(f"\n{'='*80}")
        print(f"ğŸ” Káº¾T QUáº¢ THá»¬ NGHIá»†M Äá»˜ CHÃNH XÃC TÃŒM KIáº¾M (4.3.4.2)")
        print(f"{'='*80}")
        
        if "error" in results:
            print(f"âŒ Lá»—i: {results['error']}")
            return
        
        print(f"ğŸ“Š Tá»•ng quan:")
        print(f"   â€¢ Tá»•ng cÃ¢u há»i test: {results['total_questions']}")
        print(f"   â€¢ CÃ¢u há»i thÃ nh cÃ´ng: {results['successful_questions']}")
        print(f"   â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {results['success_rate']:.1f}%")
        
        if "overall_search_metrics" in results:
            metrics = results["overall_search_metrics"]
            print(f"\nğŸ” METRIC TÃŒM KIáº¾M:")
            print("-" * 60)
            
            print(f"Precision{'':15} | {metrics['precision']['mean']:6.3f} | Äá»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m")
            print(f"Recall{'':18} | {metrics['recall']['mean']:6.3f} | Äá»™ bao phá»§ tÃ¬m kiáº¿m")
            print(f"F1-Score{'':16} | {metrics['f1_score']['mean']:6.3f} | Trung bÃ¬nh Ä‘iá»u hÃ²a")
    
    def print_multilingual_summary(self, results: Dict[str, Any]):
        """In tÃ³m táº¯t káº¿t quáº£ thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯"""
        print(f"\n{'='*80}")
        print(f"ğŸŒ Káº¾T QUáº¢ THá»¬ NGHIá»†M ÄA NGÃ”N NGá»® (4.3.4.3)")
        print(f"{'='*80}")
        
        if "error" in results:
            print(f"âŒ Lá»—i: {results['error']}")
            return
        
        print(f"ğŸ“Š Tá»•ng quan:")
        print(f"   â€¢ Tá»•ng cÃ¢u há»i test: {results['total_questions']}")
        print(f"   â€¢ CÃ¢u há»i thÃ nh cÃ´ng: {results['successful_questions']}")
        print(f"   â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {results['success_rate']:.1f}%")
        
        if "multilingual_stats" in results:
            stats = results["multilingual_stats"]
            print(f"\nğŸŒ Káº¾T QUáº¢ THEO NGÃ”N NGá»®:")
            print("-" * 60)
            
            languages = [
                ("Tiáº¿ng Viá»‡t", stats["vietnamese"]),
                ("Tiáº¿ng Anh", stats["english"]),
                ("Há»—n há»£p", stats["mixed"])
            ]
            
            for lang_name, lang_stats in languages:
                print(f"{lang_name:15} | Sá»‘ cÃ¢u: {lang_stats['count']:2d} | Äá»™ chÃ­nh xÃ¡c: {lang_stats['avg_accuracy']:5.3f} | Thá»i gian: {lang_stats['avg_response_time']/1000:6.3f}s")

async def main():
    """Cháº¡y táº¥t cáº£ cÃ¡c test Ä‘á»™ chÃ­nh xÃ¡c theo yÃªu cáº§u 4.3.4.1, 4.3.4.2, 4.3.4.3"""
    async with AccuracyTester() as tester:
        print("ğŸ¯ Báº®T Äáº¦U THá»¬ NGHIá»†M Äá»˜ CHÃNH XÃC Há»† THá»NG TRá»¢ LÃ áº¢O AN TOÃ€N THÃ”NG TIN")
        print("Theo yÃªu cáº§u 4.3.4.1, 4.3.4.2, 4.3.4.3")
        print("="*80)
        
        # Test 1: ÄÃ¡nh giÃ¡ bá»Ÿi chuyÃªn gia (4.3.4.1)
        print("\nğŸ§ª Test 1: Thá»­ nghiá»‡m Ä‘á»™ chÃ­nh xÃ¡c cÃ¢u tráº£ lá»i (50 cÃ¢u há»i chuáº©n)")
        expert_results = await tester.run_expert_evaluation_test()
        tester.print_expert_evaluation_summary(expert_results)
        tester.save_expert_evaluation_results(expert_results)
        
        # Test 2: Äá»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m (4.3.4.2)
        print("\nğŸ§ª Test 2: Thá»­ nghiá»‡m Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m (30 cÃ¢u há»i)")
        search_results = await tester.run_search_accuracy_test()
        tester.print_search_accuracy_summary(search_results)
        tester.save_search_accuracy_results(search_results)
        
        # Test 3: Thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯ (4.3.4.3)
        print("\nğŸ§ª Test 3: Thá»­ nghiá»‡m Ä‘a ngÃ´n ngá»¯ (40 cÃ¢u há»i)")
        multilingual_results = await tester.run_multilingual_test()
        tester.print_multilingual_summary(multilingual_results)
        tester.save_multilingual_results(multilingual_results)
        
        print(f"\nâœ… HOÃ€N THÃ€NH THá»¬ NGHIá»†M Äá»˜ CHÃNH XÃC")
        print("ğŸ“ CÃ¡c files káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c táº¡o:")
        print("   â€¢ JSON: expert_evaluation_test_TIMESTAMP.json")
        print("   â€¢ CSV: expert_evaluation_table_TIMESTAMP.csv")
        print("   â€¢ JSON: search_accuracy_test_TIMESTAMP.json")
        print("   â€¢ CSV: search_accuracy_table_TIMESTAMP.csv")
        print("   â€¢ JSON: multilingual_test_TIMESTAMP.json")
        print("   â€¢ CSV: multilingual_table_TIMESTAMP.csv")
        print("="*80)

if __name__ == "__main__":
    asyncio.run(main())
